{
    "meta": [
        {
            "name": "Year",
            "type": "Timeline"
        },
        {
            "name": "Authors",
            "type": "MultiSelect"
        },
        {
            "name": "Name",
            "type": "String"
        },
        {
            "name": "Bibtex",
            "type": "String"
        },
        {
            "name": "DOI",
            "type": "String"
        },
        {
            "name": "MR Devices",
            "type": "MultiSelect"
        },
        {
            "name": "2D Devices",
            "type": "MultiSelect"
        },
        {
            "name": "Configuration",
            "type": "MultiSelect"
        },
        {
            "name": "Temporal",
            "type": "MultiSelect"
        },
        {
            "name": "Relationship",
            "type": "MultiSelect"
        },
        {
            "name": "Range",
            "type": "MultiSelect"
        },
        {
            "name": "Device Dependency",
            "type": "MultiSelect"
        },
        {
            "name": "Interaction Dynamics",
            "type": "MultiSelect"
        },
        {
            "name": "Space",
            "type": "MultiSelect"
        },
        {
            "name": "Anchoring",
            "type": "MultiSelect"
        },
        {
            "name": "Use Case",
            "type": "MultiSelect"
        },
        {
            "name": "Terminology",
            "type": "MultiSelect"
        },
        {
            "name": "Main Contribution",
            "type": "MultiSelect"
        },
        {
            "name": "Secondary Contribution",
            "type": "MultiSelect"
        },
        {
            "name": "Evaluation",
            "type": "MultiSelect"
        },
        {
            "name": "Edge Case",
            "type": "MultiSelect"
        }
    ],
    "data": [
        {
            "Year": "2005",
            "Authors": [
                "Christian Sandor",
                "Alex Olwal",
                "Blaine Bell",
                "Steven Feiner"
            ],
            "Name": "Immersive mixed-reality configuration of hybrid user interfaces",
            "Bibtex": "@inproceedings{sandor2005immersive,\n  title = {Immersive Mixed-Reality Configuration of Hybrid User Interfaces},\n  booktitle = {Proceedings - {{Fourth IEEE}} and {{ACM International Symposium}} on {{Symposium}} on {{Mixed}} and {{Augmented Reality}}, {{ISMAR}} 2005},\n  author = {Sandor, Christian and Olwal, Alex and Bell, Blaine and Feiner, Steven},\n  year = {2005},\n  volume = {2005},\n  pages = {110--113},\n  publisher = {IEEE},\n  doi = {10.1109/ISMAR.2005.37},\n  abstract = {Information in hybrid user interfaces can be spread over a variety of different, but complementary, displays, with which users interact through a potentially equally varied range of interaction devices. Since the exact configuration of these displays and devices may not be known in advance, it is desirable for users to be able to reconfigure at runtime the dataflow between interaction devices and objects on the displays. To make this possible, we present the design and implementation of a prototype mixed reality system that allows users to immersively reconfigure a running hybrid user interface.},\n  isbn = {0-7695-2459-1}\n}",
            "DOI": "10.1109/ISMAR.2005.37",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2006",
            "Authors": [
                "Alexander Bornik",
                "Reinhard Beichel",
                "Dieter Schmalstieg"
            ],
            "Name": "Interactive editing of segmented volumetric datasets in a hybrid 2D/3D virtual environment",
            "Bibtex": "@inproceedings{bornik2006interactive,\n  title = {Interactive Editing of Segmented Volumetric Datasets in a Hybrid {{2D}}/{{3D}} Virtual Environment},\n  booktitle = {Proceedings of the {{ACM}} Symposium on Virtual Reality Software and Technology},\n  author = {Bornik, Alexander and Beichel, Reinhard and Schmalstieg, Dieter},\n  year = {2006},\n  series = {Vrst '06},\n  pages = {197--206},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/1180495.1180536},\n  abstract = {In this paper we present a novel system for segmentation refine-ment, which allows for interactive correction of surface models generated from imperfect automatic segmentations of arbitrary volumetric data. The proposed approach is based on a deformable surface model allowing interactive manipulation with a hybrid user interface consisting of an immersive stereoscopic display and a Tablet PC. The user interface features visualization methods and manipulation tools specifically designed for quick inspection and correction of typical defects resulting from automated segmentation of medical datasets. A number of experiments show that typical segmentation problems can be fixed within a few minutes using the system, while maintaining real-time responsiveness of the system.},\n  isbn = {1-59593-321-2}\n}",
            "DOI": "10.1145/1180495.1180536",
            "MR Devices": [
                "Stereoscopic Projection"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Medical"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Technical Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "1991",
            "Authors": [
                "Steven Feiner",
                "Ari Shamash"
            ],
            "Name": "Hybrid user interfaces: Breeding virtually bigger interfaces for physically smaller computers",
            "Bibtex": "@inproceedings{feiner1991hybrid,\n  title = {Hybrid User Interfaces: {{Breeding}} Virtually Bigger Interfaces for Physically Smaller Computers},\n  booktitle = {Proceedings of the 4th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}, {{UIST}} 1991},\n  author = {Feiner, Steven and Shamash, Ari},\n  year = {1991},\n  pages = {9--17},\n  abstract = {While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These highresolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a \"sweet spot\" within which information may be preferentially processed. We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary lowresolution surround, and a flat-panel display and mouse to provide the primary high-resolution interface.},\n  isbn = {0-89791-451-1}\n}",
            "DOI": "10.1145/120782.120783",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Theory (-)"
            ],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2009",
            "Authors": [
                "Felipe Carvalho",
                "Alberto Raposo",
                "Marcelo Gattass"
            ],
            "Name": "Designing a hybrid user interface: a case study on an oil and gas application",
            "Bibtex": "@inproceedings{decarvalho2009designing,\n  title = {Designing a Hybrid User Interface: A Case Study on an Oil and Gas Application},\n  booktitle = {Proceedings of the 8th International Conference on Virtual Reality Continuum and Its Applications in Industry},\n  author = {{de Carvalho}, Felipe Gomes and Raposo, Alberto and Gattass, Marcelo},\n  year = {2009},\n  series = {Vrcai '09},\n  pages = {191--196},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/1670252.1670293},\n  abstract = {The post-WIMP (Windows, Icons, Menu and Pointer) user interfaces are creating new interaction modalities and the use of new input and output devices. Many of these new interfaces are not yet mature, and issues related with the clear definition of an application's context and technological requirements are still under investigation. The study of the relationship between the properties of interaction devices and their influence on the performance of 3D tasks (navigation, manipulation, and selection) is an important factor in the identification of adequate setups for carrying out these tasks. Evidences of this relationship are being described by new studies on interaction tasks. However, in a broader context, each task can be decomposed into subtasks whose technological demands can be a challenge, since they require multiple interaction environments as well as transitions between them. Therefore, this work proposes a hybrid technological setup to integrate the advantages of different functional environments. In order to achieve such goal, a semi-immersive environment composed of 3 functional environments was developed and transitions between these environments were exploited during a 3D annotation task in an oil and gas application.},\n  isbn = {978-1-60558-912-1}\n}",
            "DOI": "10.1145/1670252.1670293",
            "MR Devices": [
                "CAVE"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Text Entry / Annotations"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2014",
            "Authors": [
                "Jia Wang",
                "Robert Lindeman"
            ],
            "Name": "Coordinated 3D interaction in tablet- and HMD-based hybrid virtual environments",
            "Bibtex": "@inproceedings{wang2014coordinated,\n  title = {Coordinated {{3D}} Interaction in Tablet- and {{HMD-based}} Hybrid Virtual Environments},\n  booktitle = {Proceedings of the 2nd {{ACM}} Symposium on Spatial User Interaction},\n  author = {Wang, Jia and Lindeman, Robert},\n  year = {2014},\n  series = {Sui '14},\n  pages = {70--79},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/2659766.2659777},\n  abstract = {Traditional 3D User Interfaces (3DUI) in immersive virtual reality can be inefficient in tasks that involve diversities in scale, perspective, reference frame, and dimension. This paper proposes a solution to this problem using a coordinated, tablet- and HMD-based, hybrid virtual environment system. Wearing a non-occlusive HMD, the user is able to view and interact with a tablet mounted on the non-dominant forearm, which provides a multi-touch interaction surface, as well as an exocentric God view of the virtual world. To reduce transition gaps across 3D interaction tasks and interfaces, four coordination mechanisms are proposed, two of which were implemented, and one was evaluated in a user study featuring complex level-editing tasks. Based on subjective ratings, task performance, interview feedback, and video analysis, we found that having multiple Interaction Contexts (ICs) with complementary benefits can lead to good performance and user experience, despite the complexity of learning and using the hybrid system. The results also suggest keeping 3DUI tasks synchronized across the ICs, as this can help users understand their relationships, smoothen within- and between-task IC transitions, and inspire more creative use of different interfaces.},\n  isbn = {978-1-4503-2820-3}\n}",
            "DOI": "10.1145/2659766.2659777",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Gaming",
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2015",
            "Authors": [
                "Jens Grubert",
                "Matthias Heinisch",
                "Aaron Quigley",
                "Dieter Schmalstieg"
            ],
            "Name": "MultiFi: Multi Fidelity Interaction with Displays On and Around the Body",
            "Bibtex": "@inproceedings{grubert2015multifi,\n  title = {{{MultiFi}}: {{Multi Fidelity Interaction}} with {{Displays On}} and {{Around}} the {{Body}}},\n  shorttitle = {{{MultiFi}}},\n  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Grubert, Jens and Heinisch, Matthias and Quigley, Aaron and Schmalstieg, Dieter},\n  year = {2015},\n  month = apr,\n  series = {{{CHI}} '15},\n  pages = {3933--3942},\n  publisher = {Association for Computing Machinery},\n  address = {Seoul, Republic of Korea},\n  doi = {10.1145/2702123.2702331},\n  abstract = {Display devices on and around the body such as smartwatches, head-mounted displays or tablets enable users to interact on the go. However, diverging input and output fidelities of these devices can lead to interaction seams that can inhibit efficient mobile interaction, when users employ multiple devices at once. We present MultiFi, an interactive system that combines the strengths of multiple displays and overcomes the seams of mobile interaction with widgets distributed over multiple devices. A comparative user study indicates that combined head-mounted display and smartwatch interfaces can outperform interaction with single wearable devices.},\n  isbn = {978-1-4503-3145-6}\n}",
            "DOI": "10.1145/2702123.2702331",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartwatch",
                "Smartphone"
            ],
            "Configuration": [
                "VESAD",
                "Dynamic Lens",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2015",
            "Authors": [
                "Paul Tolstoi",
                "Andreas Dippon"
            ],
            "Name": "Towering defense: An augmented reality multi-device game",
            "Bibtex": "@inproceedings{tolstoi2015towering,\n  title = {Towering Defense: {{An}} Augmented Reality Multi-Device Game},\n  booktitle = {Proceedings of the 33rd Annual {{ACM}} Conference Extended Abstracts on Human Factors in Computing Systems},\n  author = {Tolstoi, Paul and Dippon, Andreas},\n  year = {2015},\n  series = {Chi Ea '15},\n  pages = {89--92},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/2702613.2728659},\n  abstract = {This game aims to present a modern approach for interaction between multiple devices, such as a large screen tablet and a smartphone, using the example of a simple game like tower defense. The player interacts with the game on the tablet by using simple touch input and on the smartphone by using Augmented Reality and gestures. The position of the smartphone relative to the tablet is tracked by using feature tracking of the image on the tablet. Depending on the position of the smartphone the player can select different towers or different floors of a tower for additional interaction inside. Additionally selecting a resource node allows a player to mine it by executing the mining gesture with his smartphone. After selecting an enemy the user can perform gesture based interaction to deal damage directly to the selected unit.},\n  isbn = {978-1-4503-3146-3}\n}",
            "DOI": "10.1145/2702613.2728659",
            "MR Devices": [
                "Handheld AR (Smartphone)"
            ],
            "2D Devices": [
                "Tablet",
                "Smartphone"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Gaming"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2015",
            "Authors": [
                "Suzanne Mueller",
                "Andreas Dippon",
                "Gudrun Klinker"
            ],
            "Name": "Capture the flag: Engaging in a multi-device augmented reality game",
            "Bibtex": "@inproceedings{mueller2015capturea,\n  title = {Capture the Flag: {{Engaging}} in a Multi-Device Augmented Reality Game},\n  booktitle = {Proceedings of the 2015 International Conference on Interactive Tabletops \\&amp; Surfaces},\n  author = {Mueller, Suzanne and Dippon, Andreas and Klinker, Gudrun},\n  year = {2015},\n  series = {Its '15},\n  pages = {277--282},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/2817721.2823493},\n  abstract = {We present a Capture the Flag based game that investigates the possible engagements in a multi-device game. The distinction between a publicly used space and a player's private space is made and utilized to display different information to players. The tablet and the Augmented Reality component are used to see how players can be drawn to a certain physical space, to create a social and engaging game.},\n  isbn = {978-1-4503-3899-8}\n}",
            "DOI": "10.1145/2817721.2823493",
            "MR Devices": [
                "Handheld AR (Smartphone)"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Gaming"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2017",
            "Authors": [
                "Caroline Baillard",
                "Matthieu Fradet",
                "Vincent Alleaume",
                "Pierrick Jouet",
                "Anthony Laurent"
            ],
            "Name": "Multi-device mixed reality TV: a collaborative experience with joint use of a tablet and a headset",
            "Bibtex": "@inproceedings{baillard2017multidevice,\n  title = {Multi-Device Mixed Reality {{TV}}: A Collaborative Experience with Joint Use of a Tablet and a Headset},\n  booktitle = {Proceedings of the 23rd {{ACM}} Symposium on Virtual Reality Software and Technology},\n  author = {Baillard, Caroline and Fradet, Matthieu and Alleaume, Vincent and Jouet, Pierrick and Laurent, Anthony},\n  year = {2017},\n  series = {Vrst '17},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3139131.3141196},\n  abstract = {A multi-user experience extending a standard TV content with AR elements is presented. It runs with both a standard tablet and a premium MR headset, the Microsoft HoloLens. A virtual TV mosaic is displayed around the TV screen and used as a GUI to control both TV and MR content. This paper focuses on the collaborative and personalized dimension offered by the experience. Unlike most AR applications, it can be simultaneously run by several users using different devices. The users can share content with others while keeping a personalized display. The added-value of such an extended TV experience has been demonstrated through complementary types of content, and user feedback confirms a real interest in this new kind of home entertainment, at the same time immersive, interactive, collaborative and personalized.},\n  articleno = {67},\n  isbn = {978-1-4503-5548-3}\n}",
            "DOI": "10.1145/3139131.3141196",
            "MR Devices": [
                "AR OST HWD",
                "Handheld AR (Tablet)"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "VESAD",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Entertainment"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2018",
            "Authors": [
                "Patrick Reipschl\u00e4ger",
                "Burcu Ozkan Kulahcioglu",
                "Aman Mathur Shankar",
                "Stefan Gumhold",
                "Rupak Majumdar",
                "Raimund Dachselt"
            ],
            "Name": "DebugAR: Mixed Dimensional Displays for Immersive Debugging of Distributed Systems",
            "Bibtex": "@inproceedings{reipschlager2018debugar,\n  title = {{{DebugAR}}: {{Mixed Dimensional Displays}} for {{Immersive Debugging}} of {{Distributed Systems}}},\n  shorttitle = {{{DebugAR}}},\n  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '18},\n  author = {Reipschl{\\\"a}ger, Patrick and Ozkan, Burcu Kulahcioglu and Mathur, Aman Shankar and Gumhold, Stefan and Majumdar, Rupak and Dachselt, Raimund},\n  year = {2018},\n  pages = {1--6},\n  publisher = {ACM Press},\n  address = {Montreal QC, Canada},\n  doi = {10.1145/3170427.3188679},\n  isbn = {978-1-4503-5621-3},\n  langid = {english}\n}",
            "DOI": "10.1145/3170427.3188679",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Tabletop"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Fengyuan Zhu",
                "Tovi Grossman"
            ],
            "Name": "BISHARE: Exploring bidirectional interactions between smartphones and head-mounted augmented reality",
            "Bibtex": "@inproceedings{zhu2020bishare,\n  title = {{{BISHARE}}: {{Exploring}} Bidirectional Interactions between Smartphones and Head-Mounted Augmented Reality},\n  booktitle = {Proceedings of the 2020 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Zhu, Fengyuan and Grossman, Tovi},\n  year = {2020},\n  series = {Chi '20},\n  pages = {1--14},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3313831.3376233},\n  abstract = {In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.},\n  isbn = {978-1-4503-6708-0}\n}",
            "DOI": "10.1145/3313831.3376233",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Theory"
            ],
            "Secondary Contribution": [
                "Artifact (-)",
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Michael Nebeling",
                "Maximilian Speicher",
                "Xizi Wang",
                "Shwetha Rajaram",
                "Brian D. Hall",
                "Zijian Xie",
                "Alexander R. E. Raistrick",
                "Michelle Aebersold",
                "Edward G. Happ",
                "Jiayin Wang",
                "Yanan Sun",
                "Lotus Zhang",
                "Leah E. Ramsier",
                "Rhea Kulkarni"
            ],
            "Name": "MRAT: The mixed reality analytics toolkit",
            "Bibtex": "@inproceedings{nebeling2020mrat,\n  title = {{{MRAT}}: {{The}} Mixed Reality Analytics Toolkit},\n  booktitle = {Proceedings of the 2020 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Nebeling, Michael and Speicher, Maximilian and Wang, Xizi and Rajaram, Shwetha and Hall, Brian D. and Xie, Zijian and Raistrick, Alexander R. E. and Aebersold, Michelle and Happ, Edward G. and Wang, Jiayin and Sun, Yanan and Zhang, Lotus and Ramsier, Leah E. and Kulkarni, Rhea},\n  year = {2020},\n  series = {Chi '20},\n  pages = {1--12},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3313831.3376330},\n  abstract = {Significant tool support exists for the development of mixed reality (MR) applications; however, there is a lack of tools for analyzing MR experiences. We elicit requirements for future tools through interviews with 8 university research, instructional, and media teams using AR/VR in a variety of domains. While we find a common need for capturing how users perform tasks in MR, the primary differences were in terms of heuristics and metrics relevant to each project. Particularly in the early project stages, teams were uncertain about what data should, and even could, be collected with MR technologies. We designed the Mixed Reality Analytics Toolkit (MRAT) to instrument MR apps via visual editors without programming and enable rapid data collection and filtering for visualizations of MR user sessions. With MRAT, we contribute flexible interaction tracking and task definition concepts, an extensible set of heuristic techniques and metrics to measure task success, and visual inspection tools with in-situ visualizations in MR. Focusing on a multi-user, cross-device MR crisis simulation and triage training app as a case study, we then show the benefits of using MRAT, not only for user testing of MR apps, but also performance tuning throughout the design process.},\n  isbn = {978-1-4503-6708-0}\n}",
            "DOI": "10.1145/3313831.3376330",
            "MR Devices": [
                "AR OST HWD",
                "AR VST HWD",
                "VR HWD"
            ],
            "2D Devices": [
                "Tablet",
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User",
                "Multi-user - Indvidual Component"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Patrick Reipschl\u00e4ger",
                "Severin Engert",
                "Raimund Dachselt"
            ],
            "Name": "Augmented displays: Seamlessly extending interactive surfaces with head-mounted augmented reality",
            "Bibtex": "@inproceedings{reipschlager2020augmented,\n  title = {Augmented Displays: {{Seamlessly}} Extending Interactive Surfaces with Head-Mounted Augmented Reality},\n  booktitle = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},\n  author = {Reipschl{\\\"a}ger, Patrick and Engert, Severin and Dachselt, Raimund},\n  year = {2020},\n  month = apr,\n  pages = {1--4},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3334480.3383138},\n  abstract = {We present Augmented Displays, a new class of display systems combining high-resolution interactive surfaces with head-coupled Augmented Reality. This extends the screen estate beyond the display and enables placing AR content directly at the display's borders or within the real environment. Furthermore, it enables people to interact with AR objects with natural pen and touch input in high precision on the surface. This combination allows a variety of interesting applications. To illustrate them, we present two use cases: An immersive 3D modeling tool and an architectural design tool. Our goal is to demonstrate the potential of Augmented Displays as a foundation for future work in the design space of this exciting new class of systems.},\n  isbn = {978-1-4503-6819-3}\n}",
            "DOI": "10.1145/3334480.3383138",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "VESAD",
                "Augmented Display",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Augmented Display"
            ],
            "Main Contribution": [
                "Theory"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Shengzhi Wu",
                "Daragh Byrne",
                "Molly Steenson Wright"
            ],
            "Name": "\"Megereality\": Leveraging physical affordances for multi-device gestural interaction in augmented reality",
            "Bibtex": "@inproceedings{wu2020megereality,\n  title = {\"{{Megereality}}\": {{Leveraging}} Physical Affordances for Multi-Device Gestural Interaction in Augmented Reality},\n  booktitle = {Extended Abstracts of the 2020 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Wu, Shengzhi and Byrne, Daragh and Steenson, Molly Wright},\n  year = {2020},\n  series = {Chi Ea '20},\n  pages = {1--4},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3334480.3383170},\n  abstract = {We present a novel gestural interaction strategy for multi-device interactions in augmented reality (AR), in which we leverage existing physical affordances of everyday products and spaces for intuitive interactions in AR. To explore this concept, we designed and prototyped three demo scenarios: pulling virtual sticky notes from a tablet, pulling a 3D model from a computer display, and 'slurping' color from the real-world environment to smart lights with a virtual eyedropper. By merging the boundary of digital and physical, utilizing metaphors in AR and embodying the abstract process, we demonstrate an interaction strategy that harnesses the physical affordances to assist digital interaction in AR with hand gestures.},\n  isbn = {978-1-4503-6819-3}\n}",
            "DOI": "10.1145/3334480.3383170",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Tablet"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Other / None",
                "Text Entry / Annotations",
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2019",
            "Authors": [
                "Patrick Reipschl\u00e4ger",
                "Raimund Dachselt"
            ],
            "Name": "DesignAR: Immersive 3D-Modeling Combining Augmented Reality with Interactive Displays",
            "Bibtex": "@inproceedings{reipschlager2019designar,\n  title = {{{DesignAR}}: {{Immersive 3D-Modeling Combining Augmented Reality}} with {{Interactive Displays}}},\n  shorttitle = {{{DesignAR}}},\n  booktitle = {Proceedings of the 2019 {{ACM International Conference}} on {{Interactive Surfaces}} and {{Spaces}}  - {{ISS}} '19},\n  author = {Reipschl{\\\"a}ger, Patrick and Dachselt, Raimund},\n  year = {2019},\n  pages = {29--41},\n  publisher = {ACM Press},\n  address = {Daejeon, Republic of Korea},\n  doi = {10.1145/3343055.3359718},\n  isbn = {978-1-4503-6891-9},\n  langid = {english}\n}",
            "DOI": "10.1145/3343055.3359718",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "VESAD",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Augmented Display"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Rishi Vanukuru",
                "Amarnath Murugan",
                "Jayesh Pillai"
            ],
            "Name": "Dual phone AR: Using a second phone as a controller for mobile augmented reality",
            "Bibtex": "@inproceedings{vanukuru2020duala,\n  title = {Dual Phone {{AR}}: {{Using}} a Second Phone as a Controller for Mobile Augmented Reality},\n  booktitle = {Adjunct Proceedings of the 33rd Annual {{ACM}} Symposium on User Interface Software and Technology},\n  author = {Vanukuru, Rishi and Murugan, Amarnath and Pillai, Jayesh},\n  year = {2020},\n  series = {{{UIST}} '20 Adjunct},\n  pages = {117--119},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3379350.3416139},\n  abstract = {Mobile Augmented Reality applications have become increasingly popular, however the possible interactions with AR content are largely limited to on-screen gestures and spatial movement. There has been a renewed interest in designing interaction methods for mobile AR that go beyond the screen. Mobile phones present a rich range of input, output, and tracking capabilities, and have been used as controllers for Virtual Reality and head-mounted Augmented Reality applications. In this project, we explore the use of a second phone as a controller for Mobile AR. We developed ARTWO, an application that showcases Handheld Dual Phone AR through a series of small demos in which a second phone can be used to perform basic tasks such as pointing, selecting, and drawing, in the context of real use cases. We believe that the Dual Phone AR approach can help address many of the issues faced when using conventional mobile AR interactions, and also serves as a stepping stone to the general use of phones with head-mounted AR systems in the near future.},\n  isbn = {978-1-4503-7515-3}\n}",
            "DOI": "10.1145/3379350.3416139",
            "MR Devices": [
                "Handheld AR (Smartphone)"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Gaming",
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Carolin Reichherzer",
                "Jack Fraser",
                "Damien Rompapas Constantine",
                "Mark Billinghurst"
            ],
            "Name": "SecondSight: a framework for cross-device augmented reality interfaces",
            "Bibtex": "@inproceedings{reichherzer2021secondsighta,\n  title = {{{SecondSight}}: A Framework for Cross-Device Augmented Reality Interfaces},\n  booktitle = {Extended Abstracts of the 2021 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Reichherzer, Carolin and Fraser, Jack and Rompapas, Damien Constantine and Billinghurst, Mark},\n  year = {2021},\n  series = {Chi Ea '21},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3411763.3451839},\n  abstract = {This paper describes a modular framework developed to facilitate the design space exploration of cross-device Augmented Reality (AR) interfaces that combine an AR head-mounted display (HMD) with a smartphone. Currently, there is a growing interest in how AR HMDs can be used with smartphones to improve the user's AR experience. In this work, we describe a framework that enables rapid prototyping and evaluation of an interface. Our system enables different modes of interaction, content placement, and simulated AR HMD field of view to assess which combination is best suited to inform future researchers on design recommendations. We provide examples of how the framework could be used to create sample applications, the types of the studies which could be supported, and example results from a simple pilot study.},\n  articleno = {234},\n  isbn = {978-1-4503-8095-9}\n}",
            "DOI": "10.1145/3411763.3451839",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "VESAD",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Other / None",
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Ricardo Langner",
                "Marc Satkowski",
                "Wolfgang B\u00fcschel",
                "Raimund Dachselt",
                "Wolfgang Buschel",
                "Raimund Dachselt"
            ],
            "Name": "MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis",
            "Bibtex": "@inproceedings{langner2021marvis,\n  title = {{{MARVIS}}: {{Combining Mobile Devices}} and {{Augmented Reality}} for {{Visual Data Analysis}}},\n  booktitle = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},\n  author = {Langner, Ricardo and Satkowski, Marc and B{\\\"u}schel, Wolfgang and Dachselt, Raimund and Buschel, Wolfgang and Dachselt, Raimund},\n  year = {2021},\n  month = may,\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  doi = {10.1145/3411764.3445593},\n  abstract = {We present Marvis, a conceptual framework that combines mobile devices and head-mounted Augmented Reality (AR) for visual data analysis. We propose novel concepts and techniques addressing visualization-specifc challenges. By showing additional 2D and 3D information around and above displays, we extend their limited screen space. AR views between displays as well as linking and brushing are also supported, making relationships between separated visualizations plausible. We introduce the design process and rationale for our techniques. To validate Marvis' concepts and show their versatility and widespread applicability, we describe six implemented example use cases. Finally, we discuss insights from expert hands-on reviews. As a result, we contribute to a better understanding of how the combination of one or more mobile devices with AR can beneft visual data analysis. By exploring this new type of visualization environment, we hope to provide a foundation and inspiration for future mobile data visualizations.},\n  isbn = {978-1-4503-8096-6}\n}",
            "DOI": "10.1145/3411764.3445593",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "VESAD",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User",
                "Multi-user - Indvidual Component",
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Theory"
            ],
            "Secondary Contribution": [
                "Artifact (-)",
                "Empirical (-)"
            ],
            "Evaluation": [
                "Informative",
                "Usage",
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Eugenie Brasier",
                "Emmanuel Pietriga",
                "Caroline Appert"
            ],
            "Name": "AR-enhanced widgets for smartphone-centric interaction",
            "Bibtex": "@inproceedings{brasier2021arenhanced,\n  title = {{{AR-enhanced}} Widgets for Smartphone-Centric Interaction},\n  booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},\n  author = {Brasier, Eugenie and Pietriga, Emmanuel and Appert, Caroline},\n  year = {2021},\n  series = {{{MobileHCI}} '21},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3447526.3472019},\n  abstract = {We contribute a detailed investigation of AR-enhanced widgets for smartphones, where AR technology is not only used to offload widgets from the phone to the air around it, but to give users more control on input precision as well. Such widgets have the obvious benefit of freeing up screen real-estate on the phone, but their other potential benefits remain largely theoretical. Their limitations are not well understood, most particularly in terms of input performance. We compare different AR-enhanced widget designs against their state-of-the-art touch-only counterparts with a series of exploratory studies in which participants had to perform three tasks: command trigger, parameter value adjustment, and precise 2D selection. We then derive guidelines from our empirical observations.},\n  articleno = {32},\n  isbn = {978-1-4503-8328-8}\n}",
            "DOI": "10.1145/3447526.3472019",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Jan-Henrik Schr\u00f6der",
                "Daniel Schacht",
                "Niklas Peper",
                "Anita Hamurculu Marie",
                "Hans Jetter-Christian"
            ],
            "Name": "Collaborating across realities: Analytical lenses for understanding dyadic collaboration in transitional interfaces",
            "Bibtex": "@inproceedings{schroder2023collaborating,\n  title = {Collaborating across Realities: {{Analytical}} Lenses for Understanding Dyadic Collaboration in Transitional Interfaces},\n  booktitle = {Proceedings of the 2023 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Schr{\\\"o}der, Jan-Henrik and Schacht, Daniel and Peper, Niklas and Hamurculu, Anita Marie and Jetter, Hans-Christian},\n  year = {2023},\n  series = {Chi '23},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3544548.3580879},\n  abstract = {Transitional Interfaces are a yet underexplored, emerging class of cross-reality user interfaces that enable users to freely move along the reality-virtuality continuum during collaboration. To analyze and understand how such collaboration unfolds, we propose four analytical lenses derived from an exploratory study of transitional collaboration with 15 dyads. While solving a complex spatial optimization task, participants could freely switch between three contexts, each with different displays (desktop screens, tablet-based augmented reality, head-mounted virtual reality), input techniques (mouse, touch, handheld controllers), and visual representations (monoscopic and allocentric 2D/3D maps, stereoscopic egocentric views). Using the rich qualitative and quantitative data from our study, we evaluated participants' perceptions of transitional collaboration and identified commonalities and differences between dyads. We then derived four lenses including metrics and visualizations to analyze key aspects of transitional collaboration: (1) place and distance, (2) temporal patterns, (3) group use of contexts, (4) individual use of contexts.},\n  articleno = {97},\n  isbn = {978-1-4503-9421-5}\n}",
            "DOI": "10.1145/3544548.3580879",
            "MR Devices": [
                "Handheld AR (Tablet)",
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive"
            ],
            "Relationship": [
                "Multi-user - Indvidual Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Collaboration"
            ],
            "Terminology": [
                "Term: Transitional"
            ],
            "Main Contribution": [
                "Method"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Sebastian Hubenschmid",
                "Johannes Zagermann",
                "Daniel Leicht",
                "Harald Reiterer",
                "Tiare Feuchtner"
            ],
            "Name": "ARound the Smartphone: Investigating the Effects of Virtually-Extended Display Size on Spatial Memory",
            "Bibtex": "@inproceedings{hubenschmid2023smartphone,\n  title = {{{ARound}} the {{Smartphone}}: {{Investigating}} the {{Effects}} of {{Virtually-Extended Display Size}} on {{Spatial Memory}}},\n  shorttitle = {{{ARound}} the {{Smartphone}}},\n  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Hubenschmid, Sebastian and Zagermann, Johannes and Leicht, Daniel and Reiterer, Harald and Feuchtner, Tiare},\n  year = {2023},\n  month = apr,\n  pages = {1--15},\n  publisher = {ACM},\n  address = {Hamburg Germany},\n  doi = {10.1145/3544548.3581438},\n  isbn = {978-1-4503-9421-5},\n  langid = {english}\n}",
            "DOI": "10.1145/3544548.3581438",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Neil Chulpongsatorn",
                "Wesley Willett",
                "Ryo Suzuki"
            ],
            "Name": "HoloTouch: Interacting with mixed reality visualizations through smartphone proxies",
            "Bibtex": "@inproceedings{chulpongsatorn2023holotouch,\n  title = {{{HoloTouch}}: {{Interacting}} with Mixed Reality Visualizations through Smartphone Proxies},\n  booktitle = {Extended Abstracts of the 2023 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Chulpongsatorn, Neil and Willett, Wesley and Suzuki, Ryo},\n  year = {2023},\n  series = {Chi Ea '23},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3544549.3585738},\n  abstract = {We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.},\n  articleno = {156},\n  isbn = {978-1-4503-9422-2}\n}",
            "DOI": "10.1145/3544549.3585738",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Zhigeng Pan",
                "Zhipeng Pan",
                "Tianren Luo",
                "Mingmin Zhang"
            ],
            "Name": "Exploring the use of smartphones as input devices for the mixed reality environment",
            "Bibtex": "@inproceedings{pan2023exploring,\n  title = {Exploring the Use of Smartphones as Input Devices for the Mixed Reality Environment},\n  booktitle = {Proceedings of the 18th {{ACM SIGGRAPH}} International Conference on Virtual-Reality Continuum and Its Applications in Industry},\n  author = {Pan, Zhigeng and Pan, Zhipeng and Luo, Tianren and Zhang, Mingmin},\n  year = {2023},\n  series = {Vrcai '22},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3574131.3574451},\n  abstract = {Nowadays, researches on mixed reality (MR) have made a lot of exploration in the aspects of user experience, hardware devices, interaction technologies, application systems, etc. However, more research is still needed to explore how to improve the experience in the shared MR Environment and design appropriate collaborative interaction models. The traditional VR handle can realize tool simulation and 3D interaction, while smartphones can realize 2D interactions such as fast 2D gestures, text input and handwriting. Using the mobile phone as the handle of the MR headset can realize the complementary advantages of the two. In this study, we design a cross-device collaborative system sharing a hybrid reality environment. In the MR Environment, the smartphone will realize the functions of the controller for remote selection and manipulation of objects, and its advantages in 2D interaction can be well applied to some special tasks. We design the user interface with three levels of immersion: high, medium and low. High immersion: We provide a hybrid user interface combining a HoloLens2 and a smartphone. Medium immersion: We provide a 3D user interface represented by HoloLens2. Low immersion: We provide a multi-touch user interface represented by tablet. User studies show that the hybrid user interfaces can bring users satisfactory immersion and interactive experience, but it also needs to design accurate and efficient input methods according to the interactive tasks.},\n  articleno = {33},\n  isbn = {9798400700316}\n}",
            "DOI": "10.1145/3574131.3574451",
            "MR Devices": [
                "AR OST HWD",
                "Handheld AR (Smartphone)",
                "Handheld AR (Tablet)"
            ],
            "2D Devices": [],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Indvidual Component"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Collaboration"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Johann Wentzel",
                "Fraser Anderson",
                "George Fitzmaurice",
                "Tovi Grossman",
                "Daniel Vogel"
            ],
            "Name": "SwitchSpace: Understanding context-aware peeking between VR and desktop interfaces",
            "Bibtex": "@inproceedings{wentzel2024switchspace,\n  title = {{{SwitchSpace}}: {{Understanding}} Context-Aware Peeking between {{VR}} and Desktop Interfaces},\n  booktitle = {Proceedings of the {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Wentzel, Johann and Anderson, Fraser and Fitzmaurice, George and Grossman, Tovi and Vogel, Daniel},\n  year = {2024},\n  series = {Chi '24},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3613904.3642358},\n  abstract = {Cross-reality tasks, like creating or consuming virtual reality (VR) content, often involve inconvenient or distracting switches between desktop and VR. An initial formative study explores cross-reality switching habits, finding most switches are momentary ``peeks'' between interfaces, with specific habits determined by current context. The results inform a design space for context-aware ``peeking'' techniques that allow users to view or interact with desktop from VR, and vice versa, without fully switching. We implemented a set of peeking techniques and evaluated them in two levels of a cross-reality task: one requiring only viewing, and another requiring input and viewing. Peeking techniques made task completion faster, with increased input accuracy and reduced perceived workload.},\n  articleno = {801},\n  isbn = {9798400703300}\n}",
            "DOI": "10.1145/3613904.3642358",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution",
                "Migratory Interface"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Theory"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Eric Gonzalez",
                "Ishan Chatterjee",
                "Mar Gonzalez-Franco",
                "Andrea Cola\u00e7o",
                "Karan Ahuja"
            ],
            "Name": "Intent-driven input device arbitration for XR",
            "Bibtex": "@inproceedings{gonzalez2024intentdriven,\n  title = {Intent-Driven Input Device Arbitration for {{XR}}},\n  booktitle = {Extended Abstracts of the 2024 {{CHI}} Conference on Human Factors in Computing Systems},\n  author = {Gonzalez, Eric J and Chatterjee, Ishan and {Gonzalez-Franco}, Mar and Cola{\\c c}o, Andrea and Ahuja, Karan},\n  year = {2024},\n  series = {Chi Ea '24},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3613905.3650758},\n  abstract = {Interactions with Extended Reality Head-Mounted Displays (XR HMDs) require precise, intuitive, and efficient input methods. Current approaches either rely on power-intensive sensors, such as cameras for hand tracking, or specialized hardware such as controllers. Previous work has explored the use of familiar, available devices such as smartphones and smartwatches as more a more practical input alternative. However, this approach risks interaction overload -- how can one determine whether the user's gestures on the watch or phone are directed toward control of the XR device or the mobile device itself? To this end, we propose a novel method for cross-device input arbitration based on the relative orientation between the HMD and target device as measured by on-device IMUs. In a validation study with 6 users, we demonstrate 93.7\\% accuracy in estimating the intended device of interaction. Our method offers a practical, energy-efficient way to leverage users' existing devices for input and enable seamless cross-device experiences in XR.},\n  articleno = {199},\n  isbn = {9798400703317}\n}",
            "DOI": "10.1145/3613905.3650758",
            "MR Devices": [
                "AR VST HWD",
                "VR HWD"
            ],
            "2D Devices": [
                "Smartwatch",
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control",
                "Logical Distribution",
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel",
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near",
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration",
                "Technical Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Yhonatan Iquiapaza",
                "Jorge Wagner",
                "Luciana Nedel"
            ],
            "Name": "DeAR: Combining desktop and augmented reality for visual data analysis",
            "Bibtex": "@inproceedings{iquiapaza2024dear,\n  title = {{{DeAR}}: {{Combining}} Desktop and Augmented Reality for Visual Data Analysis},\n  booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},\n  author = {Iquiapaza, Yhonatan and Wagner, Jorge and Nedel, Luciana},\n  year = {2024},\n  series = {Svr '23},\n  pages = {233--237},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3625008.3625021},\n  abstract = {Combining different interfaces and displays might enhance the capabilities of data analysis, particularly in the context of Immersive Analytics. In this work, we design a prototype, called DeAR (Combining Desktop and Augmented Reality for Visual Data Analysis), which integrates a desktop environment with an augmented reality one. DeAR enables users to interact with both environments simultaneously, facilitating visualizations comparisons and diverse data analysis tasks. With our prototype, users can interact with two visualization paradigms, with changes made to visualizations in one being reflected in the other. We describe a use case scenario to demonstrate how our proposed design could facilitate exploration and selection tasks. The primary contribution of our work lies in the design and implementation of a proof-of-concept prototype, named DeAR, that allows interaction with data in hybrid environments. This prototype combines heterogeneous interfaces and displays, providing new opportunities for data analysis.},\n  isbn = {9798400709432}\n}",
            "DOI": "10.1145/3625008.3625021",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Asymmetric"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Nanjia Wang",
                "Daniel Zielasko",
                "Frank Maurer"
            ],
            "Name": "User preferences for interactive 3D object transitions in cross reality - an elicitation study",
            "Bibtex": "@inproceedings{wang2024user,\n  title = {User Preferences for Interactive {{3D}} Object Transitions in Cross Reality - an Elicitation Study},\n  booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},\n  author = {Wang, Nanjia and Zielasko, Daniel and Maurer, Frank},\n  year = {2024},\n  series = {Avi '24},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3656650.3656698},\n  abstract = {The paper investigates how users want to transfer 3D virtual objects between a standard desktop monitor and an augmented reality head-mounted display setup during a Cross Reality session. An elicitation study with 20 participants was conducted to gather their preferred interactions and input modalities to select 3D virtual objects and trigger the transition. Participants were allowed to propose interactions belonging to various input modalities or a combination of interactions that utilize multiple input modalities. We labeled the observed data and analyzed the resulting 1200 input observations to derive a set of recommended interactions. We found that mid-air hand gestures Drag, Tap, and Grab were the most commonly proposed interactions and should be considered as the primary interaction for CR virtual objects transition assisted with other interactions and input modalities as the supplementary base on different user scenarios.},\n  articleno = {22},\n  isbn = {9798400717642}\n}",
            "DOI": "10.1145/3656650.3656698",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2011",
            "Authors": [
                "Nicolas Dedual",
                "Ohan Oda",
                "Steven Feiner"
            ],
            "Name": "Creating hybrid user interfaces with a 2D multi-touch tabletop and a 3D see-through head-worn display",
            "Bibtex": "@inproceedings{dedual2011creating,\n  title = {Creating Hybrid User Interfaces with a {{2D}} Multi-Touch Tabletop and a {{3D}} See-through Head-Worn Display},\n  booktitle = {2011 10th {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}}},\n  author = {Dedual, Nicolas J. and Oda, Ohan and Feiner, Steven K.},\n  year = {2011},\n  month = oct,\n  pages = {231--232},\n  doi = {10.1109/ISMAR.2011.6092391},\n  abstract = {How can multiple different display and interaction devices be used together to create an effective augmented reality environment? We explore the design of several prototype hybrid user interfaces that combine a 2D multi-touch tabletop display with a 3D head-tracked video-see-through display. We describe a simple modeling application and an urban visualization tool in which the information presented on the head-worn display supplements the information displayed on the tabletop, using a variety of approaches to track the head-worn display relative to the tabletop. In all cases, our goal is to allow users who can see only the tabletop to interact effectively with users wearing head-worn displays.}\n}",
            "DOI": "10.1109/ISMAR.2011.6092391",
            "MR Devices": [
                "Handheld AR (Smartphone)",
                "AR VST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Sze-Wing Chan",
                "Nanjia Wang",
                "Frank Maurer"
            ],
            "Name": "Single User Cross Reality Workflow for Reservoir Engineering Work in Progress",
            "Bibtex": "@inproceedings{chan2023single,\n  title = {Single {{User Cross Reality Workflow}} for {{Reservoir Engineering Work}} in {{Progress}}},\n  booktitle = {2023 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality Adjunct}} ({{ISMAR-Adjunct}})},\n  author = {Chan, Sze-Wing and Wang, Nanjia and Maurer, Frank},\n  year = {2023},\n  month = oct,\n  pages = {211--214},\n  publisher = {IEEE},\n  address = {Sydney, Australia},\n  doi = {10.1109/ISMAR-Adjunct60411.2023.00049},\n  copyright = {https://doi.org/10.15223/policy-029},\n  isbn = {9798350328912}\n}",
            "DOI": "10.1109/ISMAR-Adjunct60411.2023.00049",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Jens Grubert",
                "Lukas Witzani",
                "Alexander Otte",
                "Travis Gesslein",
                "Matthias Kranz",
                "Per Kristensson Ola"
            ],
            "Name": "Text entry performance and situation awareness of a joint optical see-through head-mounted display and smartphone system",
            "Bibtex": "@article{grubert2023text,\n  title = {Text Entry Performance and Situation Awareness of a Joint Optical See-through Head-Mounted Display and Smartphone System},\n  author = {Grubert, Jens and Witzani, Lukas and Otte, Alexander and Gesslein, Travis and Kranz, Matthias and Kristensson, Per Ola},\n  year = {2023},\n  journal = {IEEE Transactions on Visualization and Computer Graphics},\n  pages = {1--16},\n  issn = {1941-0506},\n  doi = {10.1109/TVCG.2023.3309316},\n  abstract = {Optical see-through head-mounted displays (OST HMDs) are a popular output medium for mobile Augmented Reality (AR) applications. To date, they lack efficient text entry techniques. Smartphones are a major text entry medium in mobile contexts but attentional demands can contribute to accidents while typing on the go. Mobile multi-display ecologies, such as combined OST HMD-smartphone systems, promise performance and situation awareness benefits over single-device use. We study the joint performance of text entry on mobile phones with text output on optical see-through head-mounted displays. A series of five experiments with a total of 86 participants indicate that, as of today, the challenges in such a joint interactive system outweigh the potential benefits.}\n}",
            "DOI": "10.1109/TVCG.2023.3309316",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "VESAD",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Text Entry / Annotations"
            ],
            "Terminology": [
                "Term: Multi-Device"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Li Zhang",
                "Weiping He",
                "Zhiwei Cao",
                "Shuxia Wang",
                "Huidong Bai",
                "Mark Billinghurst"
            ],
            "Name": "Simultaneous real walking and asymmetric input in virtual reality with a smartphone-based hybrid interface",
            "Bibtex": "@inproceedings{zhang2021simultaneous,\n  title = {Simultaneous Real Walking and Asymmetric Input in Virtual Reality with a Smartphone-Based Hybrid Interface},\n  booktitle = {2021 {{IEEE}} International Symposium on Mixed and Augmented Reality Adjunct ({{ISMAR-adjunct}})},\n  author = {Zhang, Li and He, Weiping and Cao, Zhiwei and Wang, Shuxia and Bai, Huidong and Billinghurst, Mark},\n  year = {2021},\n  month = oct,\n  pages = {321--323},\n  doi = {10.1109/ISMAR-Adjunct54149.2021.00072},\n  abstract = {Compared to virtual navigation methods like joystick-based teleportation in Virtual Reality (VR), real walking enables more natural and realistic physical behaviors and better overall user experience. This paper presents a smartphone-based hybrid interface that combines a smartphone and a handheld controller, which enables real walking by viewing the physical environment and asymmetric 2D-3D input at the same time. The phone is virtualized in VR and streams a view of the real world to a collocated virtual screen, enabling users to avoid or remove physical obstacles. The touchscreen and the controller provide an asymmetric input choice for users to improve interaction efficiency in VR. We implemented a prototype system and conducted a pilot study to evaluate its usability.}\n}",
            "DOI": "10.1109/ISMAR-Adjunct54149.2021.00072",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None",
                "3D Object Manipulation"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Robbe Cools",
                "Matt Gottsacker",
                "Adalberto Simeone",
                "Gerd Bruder",
                "Greg Welch",
                "Steven Feiner"
            ],
            "Name": "Towards a desktop-AR prototyping framework: Prototyping cross-reality between desktops and augmented reality",
            "Bibtex": "@inproceedings{cools2022desktopar,\n  title = {Towards a Desktop-{{AR}} Prototyping Framework: {{Prototyping}} Cross-Reality between Desktops and Augmented Reality},\n  booktitle = {2022 {{IEEE}} International Symposium on Mixed and Augmented Reality Adjunct ({{ISMAR-adjunct}})},\n  author = {Cools, Robbe and Gottsacker, Matt and Simeone, Adalberto and Bruder, Gerd and Welch, Greg and Feiner, Steven},\n  year = {2022},\n  month = oct,\n  pages = {175--182},\n  issn = {2771-1110},\n  doi = {10.1109/ISMAR-Adjunct57072.2022.00040},\n  abstract = {Augmented reality (AR) head-worn displays (HWDs) allow users to view and interact with virtual objects anchored in the 3D space around them. These devices extend users' digital interaction space compared to traditional desktop computing environments by both allowing users to interact with a larger virtual display and by affording new interactions (e.g., intuitive 3D manipulations) with virtual content. Yet, 2D desktop displays still have advantages over AR HWDs for common computing tasks and will continue to be used well into the future. Because of their not entirely overlapping set of affordances, AR HWDs and 2D desktops may be useful in a hybrid configuration; that is, users may benefit from being able to work on computing tasks in either environment (or simultaneously in both environments) while transitioning virtual content between them. In support of such computing environments, we propose a prototyping framework for bidirectional Cross-Reality interactions between a desktop and an AR HWD. We further implemented a proof-of-concept seamless Desktop-AR display space, and describe two concrete use cases for our framework. In future work we aim to further develop our proof-of-concept into the proposed framework.}\n}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00040",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "VESAD",
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2006",
            "Authors": [
                "Alexander Bornik",
                "Reinhard Beichel",
                "Ernst Kruijff",
                "Bernhard Reitinger",
                "Dieter Schmalstieg"
            ],
            "Name": "A Hybrid User Interface for Manipulation of Volumetric Medical Data",
            "Bibtex": "@inproceedings{bornik2006hybrid,\n  title = {A {{Hybrid User Interface}} for {{Manipulation}} of {{Volumetric Medical Data}}},\n  booktitle = {{{3D User Interfaces}} ({{3DUI}}'06)},\n  author = {Bornik, A. and Beichel, R. and Kruijff, E. and Reitinger, B. and Schmalstieg, D.},\n  year = {2006},\n  month = mar,\n  pages = {29--36},\n  doi = {10.1109/VR.2006.8},\n  abstract = {This paper presents a novel system for interactive visualization and manipulation of medical datasets for surgery planning based on a hybrid VR / Tablet PC user interface. The goal of the system is to facilitate efficient visual inspection and correction of surface models generated by automated segmentation algorithms based on x-ray computed tomography scans, needed for planning surgical interventions. Factors like the quality of the visualization, nature of the dataset and interaction efficiency strongly influence system design decisions, in particular the design of the user interface, input devices and interaction techniques, leading to a hybrid setup. Finally, a user study is presented, which characterizes the system in terms of method efficiency and usability.}\n}",
            "DOI": "10.1109/VR.2006.8",
            "MR Devices": [
                "Stereoscopic Projection"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Medical",
                "SciVis"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Marium-E Jannat",
                "Khalad Hasan"
            ],
            "Name": "Exploring the effects of virtually-augmented display sizes on users\u2019 spatial memory in smartwatches",
            "Bibtex": "@inproceedings{jannat2023exploring,\n  title = {Exploring the Effects of Virtually-Augmented Display Sizes on Users' Spatial Memory in Smartwatches},\n  booktitle = {2023 {{IEEE}} International Symposium on Mixed and Augmented Reality ({{ISMAR}})},\n  author = {Jannat, Marium-E and Hasan, Khalad},\n  year = {2023},\n  month = oct,\n  pages = {553--562},\n  issn = {2473-0726},\n  doi = {10.1109/ISMAR59233.2023.00070},\n  abstract = {The small display size of the smartwatches makes it difficult to display large amounts of information on the device. Prior work explored leveraging a second device (e.g., Head-mounted displays) to extend the space where users can access large information space with virtual displays anchored on their wrists. Though researchers showed that having an additional virtual screen increased information bandwidth, little is known about the effect of virtual display sizes on users' performance. In this paper, we examined the impact of display sizes on spatial memory, workload, and user experience to better understand the prospects of virtually-augmented displays for smartwatches. Results from a user study revealed that a 4.8 inches display size can be the ``sweet spot'' for the virtually-augmented displays to ensure improved spatial memory performance and better user experience with less workload. Finally, we provided a set of design guidelines focusing to display size, spatial memory, user experience, and workload for designing virtually augmented user interfaces for smartwatches.}\n}",
            "DOI": "10.1109/ISMAR59233.2023.00070",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Smartwatch"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Eric Gonzalez",
                "Khushman Patel",
                "Karan Ahuja",
                "Mar Gonzalez-Franco"
            ],
            "Name": "XDTK: a cross-device toolkit for input & interaction in XR",
            "Bibtex": "@inproceedings{gonzalez2024xdtk,\n  title = {{{XDTK}}: A Cross-Device Toolkit for Input \\& Interaction in {{XR}}},\n  booktitle = {2024 {{IEEE}} Conference on Virtual Reality and {{3D}} User Interfaces Abstracts and Workshops ({{VRW}})},\n  author = {Gonzalez, Eric J. and Patel, Khushman and Ahuja, Karan and {Gonzalez-Franco}, Mar},\n  year = {2024},\n  month = mar,\n  pages = {467--470},\n  doi = {10.1109/VRW62533.2024.00092},\n  abstract = {We present XDTK, an open-source Unity/Android toolkit for proto-typing multi-device interactions in extended reality (XR). With the Unity package and Android app provided in XDTK, data from any number of devices (phones, tablets, or wearables) can be streamed to and surfaced within a Unity-based XR application. ARCore-supported device also provide self-tracked pose data. Devices on the same local network are automatically discovered by the Unity server and their inputs are routed using a custom event framework. We designed XDTK to be modular and easily extendable to enable fast, simple, and effective prototyping of multi-device experiences by both researchers and developers.}\n}",
            "DOI": "10.1109/VRW62533.2024.00092",
            "MR Devices": [
                "Handheld AR (Tablet)",
                "AR VST HWD",
                "AR OST HWD",
                "Handheld AR (Smartphone)",
                "VR HWD"
            ],
            "2D Devices": [
                "Smartwatch",
                "Laptop",
                "Tablet",
                "Smartphone"
            ],
            "Configuration": [
                "Migratory Interface",
                "Symmetric",
                "VESAD",
                "Logical Distribution",
                "Dynamic Lens",
                "Augmented Display",
                "Remote Control",
                "Asymmetric"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User",
                "Multi-user - Indvidual Component",
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Near",
                "Public",
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Fixed",
                "Semi-Fixed",
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)",
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free",
                "Component-coupled",
                "Dynamic"
            ],
            "Use Case": [
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Multi-Device",
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Method (-)"
            ],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2005",
            "Authors": [
                "Hrvoje Benko",
                "Edward W. Ishak",
                "Steven Feiner"
            ],
            "Name": "Cross-dimensional gestural interaction techniques for hybrid immersive environments",
            "Bibtex": "@inproceedings{benko2005crossdimensional,\n  title = {Cross-Dimensional Gestural Interaction Techniques for Hybrid Immersive Environments},\n  booktitle = {{{IEEE}} Proceedings. {{VR}} 2005. {{Virtual}} Reality, 2005.},\n  author = {Benko, H. and Ishak, E.W. and Feiner, S.},\n  year = {2005},\n  month = mar,\n  pages = {209--216},\n  issn = {2375-5334},\n  doi = {10.1109/VR.2005.1492776},\n  abstract = {We present a set of cross-dimensional interaction techniques for a hybrid user interface that integrates existing 2D and 3D visualization and interaction devices. Our approach is built around one-and two-handed gestures that support the seamless transition of data between co-located 2D and 3D contexts. Our testbed environment combines a 2D multi-user, multi-touch, projection surface with 3D head-tracked, see-through, head-worn displays and 3D tracked gloves to form a multi-display augmented reality. We address some of the ways in which we can interact with private data in a collaborative, heterogeneous workspace. We also report on a pilot usability study to evaluate the effectiveness and ease of use of the cross-dimensional interactions.}\n}",
            "DOI": "10.1109/VR.2005.1492776",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Tabletop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Tanner Hobson",
                "Jeremiah Duncan",
                "Mohammad Raji",
                "Aidong Lu",
                "Jian Huang"
            ],
            "Name": "Alpaca: AR graphics extensions for web applications",
            "Bibtex": "@inproceedings{hobson2020alpaca,\n  title = {Alpaca: {{AR}} Graphics Extensions for Web Applications},\n  booktitle = {2020 {{IEEE}} Conference on Virtual Reality and {{3D}} User Interfaces ({{VR}})},\n  author = {Hobson, Tanner and Duncan, Jeremiah and Raji, Mohammad and Lu, Aidong and Huang, Jian},\n  year = {2020},\n  month = mar,\n  pages = {174--183},\n  issn = {2642-5254},\n  doi = {10.1109/VR46266.2020.00036},\n  abstract = {In this work, we propose a framework to simplify the creation of Augmented Reality (AR) extensions for web applications, without modifying the original web applications. We implemented the framework in an open source package called Alpaca. AR extensions developed using Alpaca appear as a web-browser extension, and automatically bridge the Document Object Model (DOM) of the web with the SceneGraph model of AR. To transform the web application into a multi-device, mixed-space web application, we designed a restrictive and minimized interface for cross-device event handling. We demonstrate our approach to develop mixed-space applications using three examples. These applications are, respectively, for exploring Google Books, exploring biodiversity distribution hosted by the National Park Service of the United States, and exploring YouTube's recommendation engine. The first two cases show how a 3rd-party developer can create AR extensions without making any modifications to the original web applications. The last case serves as an example of how to create AR extensions when a developer creates a web application from scratch. Alpaca works on the iPhone X, the Google Pixel, and the Microsoft HoloLens.}\n}",
            "DOI": "10.1109/VR46266.2020.00036",
            "MR Devices": [
                "AR OST HWD",
                "Handheld AR (Smartphone)"
            ],
            "2D Devices": [
                "Laptop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Multi-Device",
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Technical Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2014",
            "Authors": [
                "Hiroyuki Kawakita",
                "Toshio Nakagawa"
            ],
            "Name": "Augmented TV: An augmented reality system for TV programs beyond the TV screen",
            "Bibtex": "@inproceedings{kawakita2014augmented,\n  title = {Augmented {{TV}}: {{An}} Augmented Reality System for {{TV}} Programs beyond the {{TV}} Screen},\n  booktitle = {2014 International Conference on Multimedia Computing and Systems ({{ICMCS}})},\n  author = {Kawakita, Hiroyuki and Nakagawa, Toshio},\n  year = {2014},\n  month = apr,\n  pages = {955--960},\n  doi = {10.1109/ICMCS.2014.6911158},\n  abstract = {TV Service using a mobile device as a second screen has been increasing. We propose a new TV system which is able to augment representation of TV programs beyond the TV screen. In the system, which we named augmented TV, since animated 3DCG content interlocked with TV programs is overlaid on live video from the mobile device camera in the mobile device screen by augmented reality techniques, the representation of having a TV character coming out of the screen can be provided. To achieve the representation giving such surprise or reality to the viewer, synchronization accuracy of the overlay display is required. A conventional synchronization method for multi-device or a visible light communication method does not meet the requirement of the accuracy. Therefore, we developed an accurate synchronization method and authoring environment of augmented TV content. We implemented augmented TV, and confirmed frame-accurate synchronization (synchronization error time is about 0.03 seconds or less). And we confirmed that the authoring environment is easy to produce augmented TV content using a video clip using a 3DCG character with TV program quality.}\n}",
            "DOI": "10.1109/ICMCS.2014.6911158",
            "MR Devices": [
                "Handheld AR (Tablet)"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "Logical Distribution",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Entertainment"
            ],
            "Terminology": [
                "Term: Augmented Display"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Technical Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Sunyoung Bang",
                "Woontack Woo"
            ],
            "Name": "Enhancing the reading experience on AR hmds by using smartphones as assistive displays",
            "Bibtex": "@inproceedings{bang2023enhancing,\n  title = {Enhancing the Reading Experience on {{AR}} Hmds by Using Smartphones as Assistive Displays},\n  booktitle = {2023 {{IEEE}} Conference Virtual Reality and {{3D}} User Interfaces ({{VR}})},\n  author = {Bang, Sunyoung and Woo, Woontack},\n  year = {2023},\n  month = mar,\n  pages = {378--386},\n  issn = {2642-5254},\n  doi = {10.1109/VR55154.2023.00053},\n  abstract = {The reading experience on current augmented reality (AR) head mounted displays (HMDs) is often impeded by the devices' low perceived resolution, translucency, and small field of view, especially in situations involving lengthy text. Although many researchers have proposed methods to resolve this issue, the inherent characteristics prevent these displays from delivering a readability on par with that of more traditional displays. As a solution, we explore the use of smartphones as assistive displays to AR HMDs. To validate the feasibility of our approach, we conducted a user study in which we compared a smartphone-assisted hybrid interface against using the HMD only for two different text lengths. The results demonstrate that the hybrid interface yields a lower task load regardless of the text length, although it does not improve task performance. Furthermore, the hybrid interface provides a better experience regarding user comfort, visual fatigue, and perceived readability. Based on these results, we claim that joining the spatial output capabilities of the HMD with the high-resolution display of the smartphone is a viable solution for improving the reading experience in AR.}\n}",
            "DOI": "10.1109/VR55154.2023.00053",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Text Entry / Annotations"
            ],
            "Terminology": [
                "Term: Multi-Device",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2018",
            "Authors": [
                "Carmine Elvezio",
                "Pierre Amelot",
                "Robert Boyle",
                "Catherine Wes Ilona",
                "Steven Feiner"
            ],
            "Name": "Hybrid uis for music exploration in AR and VR",
            "Bibtex": "@inproceedings{elvezio2018hybrid,\n  title = {Hybrid Uis for Music Exploration in {{AR}} and {{VR}}},\n  booktitle = {2018 {{IEEE}} International Symposium on Mixed and Augmented Reality Adjunct ({{ISMAR-adjunct}})},\n  author = {Elvezio, Carmine and Amelot, Pierre and Boyle, Robert and Wes, Catherine Ilona and Feiner, Steven},\n  year = {2018},\n  month = oct,\n  pages = {411--412},\n  doi = {10.1109/ISMAR-Adjunct.2018.00121},\n  abstract = {We present hybrid user interfaces that facilitate interaction with music content in 3D, using a combination of 2D and 3D input and display devices. Participants will explore an online music library, some wearing AR or VR head-worn displays used alone or in conjunction with touch screens, and others using only touch screens. They will select genres, artists, albums and songs, interacting through a combination of 3D hand-tracking and 2D multi-touch technologies.}\n}",
            "DOI": "10.1109/ISMAR-Adjunct.2018.00121",
            "MR Devices": [
                "AR OST HWD",
                "VR HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Entertainment"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Akira Kanaoka",
                "Takamasa Isohara"
            ],
            "Name": "Enhancing smishing detection in AR environments: Cross-device solutions for seamless reality",
            "Bibtex": "@inproceedings{kanaoka2024enhancing,\n  title = {Enhancing Smishing Detection in {{AR}} Environments: {{Cross-device}} Solutions for Seamless Reality},\n  booktitle = {2024 {{IEEE}} Conference on Virtual Reality and {{3D}} User Interfaces Abstracts and Workshops ({{VRW}})},\n  author = {Kanaoka, Akira and Isohara, Takamasa},\n  year = {2024},\n  month = mar,\n  pages = {565--572},\n  doi = {10.1109/VRW62533.2024.00108},\n  abstract = {Amidst the evolving landscape where cyber and physical realms seamlessly intertwine, smishing (SMS phishing) emerges as a so-phisticated security threat. This paper introduces a novel cross-device solution for detecting smishing attacks in augmented reality (AR) environments. Our method uses AR glasses to enhance user perception and interaction by analyzing images of URLs displayed across various devices, including smartphones, laptops, and digital displays. This approach aligns with the ``Seamless Reality'' concept, integrating physical and cyber space perception, cognition, and interaction. We detail the development of a prototype system and present findings from a user study that evaluates its effectiveness in a diverse range of AR settings. Our results underscore the potential of AR technologies in smishing detection, contributing to a safer interaction between virtual and real-world elements. The study also delves into aspects of AR display systems, user interface design, and the integration of haptic feedback, offering insights into the broader implications for seamless AR experiences.}\n}",
            "DOI": "10.1109/VRW62533.2024.00108",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "1999",
            "Authors": [
                "Andreas Butz",
                "Tobias H\u00f6llerer",
                "Steven Feiner",
                "Blair MacIntyre",
                "Clifford Beshers"
            ],
            "Name": "Enveloping users and computers in a collaborative 3D augmented reality",
            "Bibtex": "@inproceedings{butz1999enveloping,\n  title = {Enveloping Users and Computers in a Collaborative {{3D}} Augmented Reality},\n  booktitle = {Proceedings 2nd {{IEEE}} and {{ACM}} International Workshop on Augmented Reality ({{IWAR}}'99)},\n  author = {Butz, A. and Hollerer, T. and Feiner, S. and MacIntyre, B. and Beshers, C.},\n  year = {1999},\n  month = oct,\n  pages = {35--44},\n  doi = {10.1109/IWAR.1999.803804},\n  abstract = {We present EMMIE (Environment Management for Multiuser Information Environments), a prototype experimental user interface to a collaborative augmented environment. Users share a 3D virtual space and manipulate virtual objects that represent information to be discussed. We refer to EMMIE as a hybrid user interface because it combines a variety of different technologies and techniques, including virtual elements such as 3D widgets, and physical objects such as tracked displays and input devices. See-through head-worn displays overlay the virtual environment on the physical environment, visualizing the pervasive \"virtual ether\" within which all interaction occurs. Our prototype includes additional 2D and 3D displays, ranging from palm-sized to wall-sized, allowing the most appropriate one to be used for any task. Objects can be moved among displays (including across dimensionalities) through drag-and-drop. In analogy to 2D window managers, we describe a prototype implementation of a shared 3D environment manager that is distributed across displays, machines, and operating systems. We also discuss two methods we are exploring for handling information privacy in such an environment.}\n}",
            "DOI": "10.1109/IWAR.1999.803804",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Tablet",
                "Projection",
                "Laptop"
            ],
            "Configuration": [
                "Asymmetric",
                "Augmented Display",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Collaboration"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2012",
            "Authors": [
                "Felipe Carvalho",
                "Daniela Trevisan",
                "Alberto Raposo"
            ],
            "Name": "Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface",
            "Bibtex": "@article{carvalho2012design,\n  title = {Toward the Design of Transitional Interfaces: An Exploratory Study on a Semi-Immersive Hybrid User Interface},\n  shorttitle = {Toward the Design of Transitional Interfaces},\n  author = {Carvalho, Felipe G. and Trevisan, Daniela G. and Raposo, Alberto},\n  year = {2012},\n  month = nov,\n  journal = {Virtual Reality},\n  volume = {16},\n  number = {4},\n  pages = {271--288},\n  issn = {1434-9957},\n  doi = {10.1007/s10055-011-0205-y},\n  abstract = {A task that can be decomposed into subtasks with different technological demands may be a challenge, since it requires multiple interactive environments as well as transitions between them. Some of these transitions may involve changes in hardware devices and interface paradigms at the same time. Some previous works have proposed various setups for hybrid user interfaces, but none of them focused on the design of transition interactions. Our work emphasizes the importance of interaction continuity as a guideline in the design and evaluation of transitional interfaces within a hybrid user interface (HUI). Finally, an exploratory study demonstrates how this design aspect is perceived by users during transitions in an HUI composed by three interactive environments.},\n  langid = {english}\n}",
            "DOI": "10.1007/s10055-011-0205-y",
            "MR Devices": [
                "CAVE"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Transitional",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Robin Fischer",
                "Wei-Xiang Lian",
                "Shiann-Jang Wang",
                "Wei-En Hsu",
                "Li-Chen Fu"
            ],
            "Name": "Seamless Virtual Object Transitions: Enhancing User Experience in Cross-Device Augmented Reality Environments",
            "Bibtex": "@incollection{fischer2023seamless,\n  title = {Seamless {{Virtual Object Transitions}}: {{Enhancing User Experience}} in {{Cross-Device Augmented Reality Environments}}},\n  shorttitle = {Seamless {{Virtual Object Transitions}}},\n  booktitle = {Extended {{Reality}}},\n  author = {Fischer, Robin and Lian, Wei-Xiang and Wang, Shiann-Jang and Hsu, Wei-En and Fu, Li-Chen},\n  editor = {De Paolis, Lucio Tommaso and Arpaia, Pasquale and Sacco, Marco},\n  year = {2023},\n  volume = {14218},\n  pages = {397--409},\n  publisher = {Springer Nature Switzerland},\n  address = {Cham},\n  doi = {10.1007/978-3-031-43401-3_26},\n  isbn = {978-3-031-43400-6 978-3-031-43401-3},\n  langid = {english}\n}",
            "DOI": "10.1007/978-3-031-43401-3_26",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Projection",
                "Smartphone"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Patrick Reipschl\u00e4ger",
                "Tamara Flemisch",
                "Raimund Dachselt"
            ],
            "Name": "Personal Augmented Reality for Information Visualization on Large Interactive Displays",
            "Bibtex": "@article{reipschlager2021personal,\n  title = {Personal {{Augmented Reality}} for {{Information Visualization}} on {{Large Interactive Displays}}},\n  author = {Reipschl{\\\"a}ger, Patrick and Flemisch, Tamara and Dachselt, Raimund},\n  year = {2021},\n  journal = {IEEE Transactions on Visualization and Computer Graphics},\n  volume = {27},\n  number = {2},\n  pages = {1182--1192},\n  issn = {1941-0506},\n  doi = {10.1109/TVCG.2020.3030460}\n}",
            "DOI": "10.1109/TVCG.2020.3030460",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User",
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Other Terms",
                "Term: Augmented Display"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "1999",
            "Authors": [
                "Jun Rekimoto",
                "Masanori Saitoh"
            ],
            "Name": "Augmented surfaces: a spatially continuous work space for hybrid computing environments",
            "Bibtex": "@article{rekimoto1999augmented,\n  title = {Augmented Surfaces: A Spatially Continuous Work Space for Hybrid Computing Environments},\n  author = {Rekimoto, J and Saitoh, M},\n  year = {1999},\n  journal = {Proceedings of the SIGCHI conference on Human {\\dots}},\n  publisher = {dl.acm.org},\n  doi = {10.1145/302979.303113},\n  abstract = {This paper describes our design and implementation of a computer augmented environment that allows users to smoothly interchange digital information among their portable {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.1145/302979.303113",
            "MR Devices": [
                "Spatial AR (projector based)"
            ],
            "2D Devices": [
                "Projection",
                "Laptop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component",
                "Multi-user - Indvidual Component"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Collaboration"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Other Terms"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Sebastian Hubenschmid",
                "Jonathan Wieland",
                "Daniel Fink Immanuel",
                "Andrea Batch",
                "Johannes Zagermann",
                "Niklas Elmqvist",
                "Harald Reiterer"
            ],
            "Name": "ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies",
            "Bibtex": "@inproceedings{hubenschmid2022relive,\n  title = {{{ReLive}}: {{Bridging In-Situ}} and {{Ex-Situ Visual Analytics}} for {{Analyzing Mixed Reality User Studies}}},\n  shorttitle = {{{ReLive}}},\n  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Hubenschmid, Sebastian and Wieland, Jonathan and Fink, Daniel Immanuel and Batch, Andrea and Zagermann, Johannes and Elmqvist, Niklas and Reiterer, Harald},\n  year = {2022},\n  month = apr,\n  series = {{{CHI}} '22},\n  pages = {1--20},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3491102.3517550},\n  abstract = {The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.},\n  isbn = {978-1-4503-9157-3}\n}",
            "DOI": "10.1145/3491102.3517550",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Asymmetric",
                "Migratory Interface",
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "1999",
            "Authors": [
                "Tobias H\u00f6llerer",
                "Steven Feiner",
                "Tachio Terauchi",
                "Gus Rashid",
                "Drexel Hallaway"
            ],
            "Name": "Exploring MARS: developing indoor and outdoor user interfaces to a mobile augmented reality system",
            "Bibtex": "@article{hollerer1999exploring,\n  type = {{{HTML}}},\n  title = {Exploring {{MARS}}: Developing Indoor and Outdoor User Interfaces to a Mobile Augmented Reality System},\n  author = {H{\\\"o}llerer, T and Feiner, S and Terauchi, T and Rashid, G and {...}},\n  year = {1999},\n  journal = {Computers \\& {\\dots}},\n  publisher = {Elsevier},\n  abstract = {We describe an experimental mobile augmented reality system (MARS) testbed that employs different user interfaces to allow outdoor and indoor users to access and manage {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.1016/S0097-8493(99)00103-X",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Tablet"
            ],
            "Configuration": [
                "Asymmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Sebastian Hubenschmid",
                "Johannes Zagermann",
                "Simon Butscher",
                "Harald Reiterer"
            ],
            "Name": "Stream: Exploring the combination of spatially-aware tablets with augmented reality head-mounted displays for immersive analytics",
            "Bibtex": "@inproceedings{hubenschmid2021stream,\n  title = {Stream: {{Exploring}} the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics},\n  booktitle = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},\n  author = {Hubenschmid, Sebastian and Zagermann, Johannes and Butscher, Simon and Reiterer, Harald},\n  year = {2021},\n  month = may,\n  publisher = {Association for Computing Machinery},\n  doi = {10.1145/3411764.3445298},\n  abstract = {Recent research in the area of immersive analytics demonstrated the utility of head-mounted augmented reality devices for visual data analysis. However, it can be challenging to use the by default supported mid-air gestures to interact with visualizations in augmented reality (e.g. due to limited precision). Touch-based interaction (e.g. via mobile devices) can compensate for these drawbacks, but is limited to two-dimensional input. In this work we present STREAM: Spatially-aware Tablets combined with Augmented Reality Head-Mounted Displays for the multimodal interaction with 3D visualizations. We developed a novel eyes-free interaction concept for the seamless transition between the tablet and the augmented reality environment. A user study reveals that participants appreciated the novel interaction concept, indicating the potential for spatially-aware tablets in augmented reality. Based on our fndings, we provide design insights to foster the application of spatially-aware touch devices in augmented reality and research implications indicating areas that need further investigation.},\n  isbn = {978-1-4503-8096-6}\n}",
            "DOI": "10.1145/3411764.3445298",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Symmetric",
                "Logical Distribution",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Daniel Schwajda",
                "Judith Friedl",
                "Fabian Pointecker",
                "Hans-Christian Jetter",
                "Christoph Anthes"
            ],
            "Name": "Transforming graph data visualisations from 2D displays into augmented reality 3D space: A quantitative study",
            "Bibtex": "@article{schwajda2023transforming,\n  type = {{{HTML}}},\n  title = {Transforming Graph Data Visualisations from {{2D}} Displays into Augmented Reality {{3D}} Space: {{A}} Quantitative Study},\n  author = {Schwajda, D and Friedl, J and Pointecker, F and Jetter, {\\relax HC} and {...}},\n  year = {2023},\n  journal = {Frontiers in Virtual {\\dots}},\n  publisher = {frontiersin.org},\n  doi = {10.3389/frvir.2023.1155628},\n  abstract = {Modern video-based head-mounted displays allow users to operate along Milgram's entire reality-virtuality continuum. This opens up the field for novel cross-reality applications that {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.3389/frvir.2023.1155628",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "Asymmetric",
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Theory (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Rapha\u00ebl James",
                "Anastasia Bezerianos",
                "Olivier Chapuis"
            ],
            "Name": "Evaluating the extension of wall displays with AR for collaborative work",
            "Bibtex": "@article{james2023evaluating,\n  title = {Evaluating the Extension of Wall Displays with {{AR}} for Collaborative Work},\n  author = {James, R and Bezerianos, A and Chapuis, O},\n  year = {2023},\n  journal = {{\\dots} of the 2023 CHI Conference on {\\dots}},\n  publisher = {dl.acm.org},\n  doi = {10.1145/3544548.3580752},\n  abstract = {Wall displays are well suited for collaborative work and are often placed in rooms with ample space in front of them that remains largely unused. Augmented Reality (AR) headsets can {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.1145/3544548.3580752",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Collaboration"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2013",
            "Authors": [
                "Rahul Budhiraja",
                "Gun A. Lee",
                "Mark Billinghurst"
            ],
            "Name": "Using a HHD with a HMD for mobile AR interaction",
            "Bibtex": "@article{budhiraja2013using,\n  title = {Using a {{HHD}} with a {{HMD}} for Mobile {{AR}} Interaction},\n  author = {Budhiraja, R and Lee, {\\relax GA} and {...}},\n  year = {2013},\n  journal = {2013 IEEE International {\\dots}},\n  publisher = {ieeexplore.ieee.org},\n  abstract = {Mobile Augmented Reality (AR) applications are typically deployed either on head mounted displays (HMD) or handheld displays (HHD). This paper explores novel interaction {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.1109/ISMAR.2013.6671837",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Katja Vock",
                "Sebastian Hubenschmid",
                "Johannes Zagermann",
                "Simon Butscher",
                "Harald Reiterer"
            ],
            "Name": "IDIAR: Augmented Reality Dashboards to Supervise Mobile Intervention Studies",
            "Bibtex": "@inproceedings{vock2021idiar,\n  title = {{{IDIAR}}: {{Augmented Reality Dashboards}} to {{Supervise Mobile Intervention Studies}}},\n  booktitle = {{{ACM International Conference Proceeding Series}}},\n  author = {Vock, Katja and Hubenschmid, Sebastian and Zagermann, Johannes and Butscher, Simon and Reiterer, Harald},\n  year = {2021},\n  pages = {248--259},\n  doi = {10.1145/3473856.3473876},\n  abstract = {Mobile intervention studies employ mobile devices to observe participants' behavior change over several weeks. Researchers regularly monitor high-dimensional data streams to ensure data quality and prevent data loss (e.g., missing engagement or malfunctions). The multitude of problem sources hampers possible automated detection of such irregularities - providing a use case for interactive dashboards. With the advent of untethered head-mounted AR devices, these dashboards can be placed anywhere in the user's physical environment, leveraging the available space and allowing for flexible information arrangement and natural navigation. In this work, we present the user-centered design and the evaluation of IDIAR: Interactive Dashboards in AR, combining a head-mounted display with the familiar interaction of a smartphone. A user study with 15 domain experts for mobile intervention studies shows that participants appreciated the multimodal interaction approach. Based on our findings, we provide implications for research and design of interactive dashboards in AR.},\n  isbn = {978-1-4503-8645-6}\n}",
            "DOI": "10.1145/3473856.3473876",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Michael Aichem",
                "Karsten Klein",
                "Tobias Czauderna",
                "Dimitar Garkov",
                "Jinxin Zhao",
                "Jian Li",
                "Falk Schreiber"
            ],
            "Name": "Towards a hybrid user interface for the visual exploration of large biomolecular networks using virtual reality",
            "Bibtex": "@article{aichem2022hybrid,\n  title = {Towards a Hybrid User Interface for the Visual Exploration of Large Biomolecular Networks Using Virtual Reality},\n  author = {Aichem, Michael and Klein, Karsten and Czauderna, Tobias and Garkov, Dimitar and Zhao, Jinxin and Li, Jian and Schreiber, Falk},\n  year = {2022},\n  month = dec,\n  journal = {Journal of Integrative Bioinformatics},\n  volume = {19},\n  number = {4},\n  publisher = {De Gruyter},\n  issn = {1613-4516},\n  doi = {10.1515/jib-2022-0034},\n  abstract = {Biomolecular networks, including genome-scale metabolic models (GSMMs), assemble the knowledge regarding the biological processes that happen inside specific organisms in a way that allows for analysis, simulation, and exploration. With the increasing availability of genome annotations and the development of powerful reconstruction tools, biomolecular networks continue to grow ever larger. While visual exploration can facilitate the understanding of such networks, the network sizes represent a major challenge for current visualisation systems. Building on promising results from the area of immersive analytics, which among others deals with the potential of immersive visualisation for data analysis, we present a concept for a hybrid user interface that combines a classical desktop environment with a virtual reality environment for the visual exploration of large biomolecular networks and corresponding data. We present system requirements and design considerations, describe a resulting concept, an envisioned technical realisation, and a systems biology usage scenario. Finally, we discuss remaining challenges.},\n  langid = {english}\n}",
            "DOI": "10.1515/jib-2022-0034",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Exclusive"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2004",
            "Authors": [
                "Adrian Ilie",
                "Kok-Lim Low",
                "Greg Welch",
                "Anselmo Lastra",
                "Henry Fuchs",
                "Bruce Cairns"
            ],
            "Name": "Combining head-mounted and projector-based displays for surgical training",
            "Bibtex": "@article{ilie2004combining,\n  title = {Combining Head-Mounted and Projector-Based Displays for Surgical Training},\n  author = {Ilie, A and Low, {\\relax KL} and Welch, G and Lastra, A and Fuchs, H and {...}},\n  year = {2004},\n  journal = {{\\dots} Teleoperators \\&Virtual {\\dots}},\n  publisher = {direct.mit.edu},\n  abstract = {We introduce and present preliminary results for a hybrid display system combining head-mounted and projector-based displays. Our work is motivated by a surgical training {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.1109/VR.2003.1191128",
            "MR Devices": [
                "Spatial AR (projector based)",
                "VR HWD"
            ],
            "2D Devices": [
                "Projection"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Medical"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2007",
            "Authors": [
                "Sangyoon Lee",
                "Sunghoon Yim",
                "Gerard Jounghyun Kim",
                "Ungyeon Yang",
                "Chang-Hun Kim"
            ],
            "Name": "Design and evaluation of a hybrid display system for motion-following tasks",
            "Bibtex": "@article{lee2007design,\n  title = {Design and Evaluation of a Hybrid Display System for Motion-Following Tasks},\n  author = {Lee, S and Yim, S and Kim, {\\relax GJ} and Yang, U and Kim, {\\relax CH}},\n  year = {2007},\n  journal = {{\\dots} , ICVR 2007, Held as part of HCI {\\dots}},\n  publisher = {Springer},\n  doi = {10.1007/978-3-540-73335-5_31},\n  abstract = {Hybrid display systems are those that combine different types of displays to exploit the complementary characteristics of the constituent display systems. In this paper, we introduce {\\dots}},\n  note+duplicate-1 = {Query date: 2024-06-17 08:22:08}\n}",
            "DOI": "10.1007/978-3-540-73335-5_31",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Projection"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Marc Satkowski",
                "Weizhou Luo",
                "Raimund Dachselt"
            ],
            "Name": "Towards In-situ Authoring of AR Visualizations with Mobile Devices",
            "Bibtex": "@inproceedings{satkowski2021insitu,\n  title = {Towards {{In-situ Authoring}} of {{AR Visualizations}} with {{Mobile Devices}}},\n  booktitle = {2021 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality Adjunct}} ({{ISMAR-Adjunct}})},\n  author = {Satkowski, Marc and Luo, Weizhou and Dachselt, Raimund},\n  year = {2021},\n  month = oct,\n  pages = {324--325},\n  publisher = {IEEE},\n  doi = {10.1109/ISMAR-Adjunct54149.2021.00073},\n  isbn = {978-1-66541-298-8}\n}",
            "DOI": "10.1109/ISMAR-Adjunct54149.2021.00073",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2009",
            "Authors": [
                "Shaun K. Kane",
                "Daniel Avrahami",
                "Jacob O. Wobbrock",
                "Beverly Harrison",
                "Adam D. Rea",
                "Matthai Philipose",
                "Anthony LaMarca"
            ],
            "Name": "Bonfire: a nomadic system for hybrid laptop-tabletop interaction",
            "Bibtex": "@inproceedings{kane2009bonfire,\n  title = {Bonfire: A Nomadic System for Hybrid Laptop-Tabletop Interaction},\n  shorttitle = {Bonfire},\n  booktitle = {Proceedings of the 22nd Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},\n  author = {Kane, Shaun K. and Avrahami, Daniel and Wobbrock, Jacob O. and Harrison, Beverly and Rea, Adam D. and Philipose, Matthai and LaMarca, Anthony},\n  year = {2009},\n  month = oct,\n  series = {{{UIST}} '09},\n  pages = {129--138},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/1622176.1622202},\n  abstract = {We present Bonfire, a self-contained mobile computing system that uses two laptop-mounted laser micro-projectors to project an interactive display space to either side of a laptop keyboard. Coupled with each micro-projector is a camera to enable hand gesture tracking, object recognition, and information transfer within the projected space. Thus, Bonfire is neither a pure laptop system nor a pure tabletop system, but an integration of the two into one new nomadic computing platform. This integration (1) enables observing the periphery and responding appropriately, e.g., to the casual placement of objects within its field of view, (2) enables integration between physical and digital objects via computer vision, (3) provides a horizontal surface in tandem with the usual vertical laptop display, allowing direct pointing and gestures, and (4) enlarges the input/output space to enrich existing applications. We describe Bonfire's architecture, and offer scenarios that highlight Bonfire's advantages. We also include lessons learned and insights for further development and use.},\n  isbn = {978-1-60558-745-5}\n}",
            "DOI": "10.1145/1622176.1622202",
            "MR Devices": [
                "Spatial AR (projector based)"
            ],
            "2D Devices": [
                "Projection",
                "Laptop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Marc Satkowski",
                "Rufat Rzayev",
                "Eva Goebel",
                "Raimund Dachselt"
            ],
            "Name": "ABOVE & BELOW: Investigating Ceiling and Floor for Augmented Reality Content Placement",
            "Bibtex": "@inproceedings{satkowski2022investigating,\n  title = {{{ABOVE}} \\& {{BELOW}}: {{Investigating Ceiling}} and {{Floor}} for {{Augmented Reality Content Placement}}},\n  shorttitle = {{{ABOVE}} \\& {{BELOW}}},\n  booktitle = {2022 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},\n  author = {Satkowski, Marc and Rzayev, Rufat and Goebel, Eva and Dachselt, Raimund},\n  year = {2022},\n  month = oct,\n  pages = {518--527},\n  issn = {1554-7868},\n  doi = {10.1109/ISMAR55827.2022.00068},\n  abstract = {Augmented Reality (AR) interfaces support users by providing access to digital content within real-world environments. However, displaying content at the users' eye level might result in the occlusion of the real world. Therefore, it requires finding AR content placement areas that free the users' field of vision. In this work, we systematically investigate two content placement areas beyond the users' eye level: the ceiling and floor. To understand how potential users perceive virtual content on the ceiling and floor and how the content should be placed on these areas, we conducted two user studies. While the first exploratory study showed the general usefulness of either area, the second quantitative study allowed us to define optimal placement parameters regarding visibility and comfort. With insights from our studies, we provide design recommendations for future AR applications that support 2D content presentation on the ceiling and the floor.}\n}",
            "DOI": "10.1109/ISMAR55827.2022.00068",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Study"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Leonardo Pavanatto",
                "Feiyu Lu",
                "Chris North",
                "Doug A. Bowman"
            ],
            "Name": "Multiple Monitors or Single Canvas? Evaluating Window Management and Layout Strategies on Virtual Displays",
            "Bibtex": "@article{pavanatto2024multiple,\n  title = {Multiple {{Monitors}} or {{Single Canvas}}? {{Evaluating Window Management}} and {{Layout Strategies}} on {{Virtual Displays}}},\n  shorttitle = {Multiple {{Monitors}} or {{Single Canvas}}?},\n  author = {Pavanatto, Leonardo and Lu, Feiyu and North, Chris and Bowman, Doug A.},\n  year = {2024},\n  journal = {IEEE Transactions on Visualization and Computer Graphics},\n  pages = {1--15},\n  issn = {1077-2626, 1941-0506, 2160-9306},\n  doi = {10.1109/TVCG.2024.3368930},\n  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}\n}",
            "DOI": "10.1109/TVCG.2024.3368930",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Leonardo Pavanatto",
                "Chris North",
                "Doug A. Bowman",
                "Carmen Badea",
                "Richard Stoakley"
            ],
            "Name": "Do we still need physical monitors? An evaluation of the usability of AR virtual monitors for productivity work",
            "Bibtex": "@inproceedings{pavanatto2021we,\n  title = {Do We Still Need Physical Monitors? {{An}} Evaluation of the Usability of {{AR}} Virtual Monitors for Productivity Work},\n  shorttitle = {Do We Still Need Physical Monitors?},\n  booktitle = {2021 {{IEEE Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},\n  author = {Pavanatto, Leonardo and North, Chris and Bowman, Doug A. and Badea, Carmen and Stoakley, Richard},\n  year = {2021},\n  month = mar,\n  pages = {759--767},\n  publisher = {IEEE},\n  address = {Lisboa, Portugal},\n  doi = {10.1109/VR50410.2021.00103},\n  isbn = {978-1-66541-838-6}\n}",
            "DOI": "10.1109/VR50410.2021.00103",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2018",
            "Authors": [
                "Rahul Arora",
                "Habib Rubaiat Kazi",
                "Tovi Grossman",
                "George Fitzmaurice",
                "Karan Singh"
            ],
            "Name": "SymbiosisSketch: Combining 2D & 3D Sketching for Designing Detailed 3D Objects in Situ",
            "Bibtex": "@inproceedings{arora2018symbiosissketch,\n  title = {{{SymbiosisSketch}}: {{Combining 2D}} \\& {{3D Sketching}} for {{Designing Detailed 3D Objects}} in {{Situ}}},\n  shorttitle = {{{SymbiosisSketch}}},\n  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Arora, Rahul and Habib Kazi, Rubaiat and Grossman, Tovi and Fitzmaurice, George and Singh, Karan},\n  year = {2018},\n  month = apr,\n  series = {{{CHI}} '18},\n  pages = {1--15},\n  publisher = {Association for Computing Machinery},\n  address = {Montreal QC, Canada},\n  doi = {10.1145/3173574.3173759},\n  abstract = {We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air.},\n  isbn = {978-1-4503-5620-6}\n}",
            "DOI": "10.1145/3173574.3173759",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Symmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2019",
            "Authors": [
                "Christopher Berns",
                "Grace Chin",
                "Joel Savitz",
                "Jason Kiesling",
                "Fred Martin"
            ],
            "Name": "MYR: A Web-Based Platform for Teaching Coding Using VR",
            "Bibtex": "@inproceedings{berns2019myr,\n  title = {{{MYR}}: {{A Web-Based Platform}} for {{Teaching Coding Using VR}}},\n  shorttitle = {{{MYR}}},\n  booktitle = {Proceedings of the 50th {{ACM Technical Symposium}} on {{Computer Science Education}}},\n  author = {Berns, Christopher and Chin, Grace and Savitz, Joel and Kiesling, Jason and Martin, Fred},\n  year = {2019},\n  month = feb,\n  series = {{{SIGCSE}} '19},\n  pages = {77--83},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3287324.3287482},\n  abstract = {MYR is a browser-based, educational platform built to spark student interest in computer science by allowing users to write code that generates three-dimensional, animated scenes in virtual reality. The interface consists of two primary components: (1) an integrated editor, which leverages the MYR API and the A-Frame entity-component-system, and (2) a real-time renderer that displays the corresponding scene. The scenes, which vary in complexity, are viewable using virtual reality headsets, smartphones, and any device that supports a web browser. By providing access to the specific domain of virtual reality to students, the system aims to make computer science concepts tangible for novice programmers. The MYR development team conducted pilot tests with middle school students in order to collect feedback from this audience. The larger goal of the project is to develop MYR as a research tool to gain insight into computing students' success, motivation, and confidence in learning computing.},\n  isbn = {978-1-4503-5890-3}\n}",
            "DOI": "10.1145/3287324.3287482",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Laptop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Development/Authoring"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Tobias Drey",
                "Jan Gugenheimer",
                "Julian Karlbauer",
                "Maximilian Milo",
                "Enrico Rukzio"
            ],
            "Name": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality",
            "Bibtex": "@inproceedings{drey2020vrsketchin,\n  title = {{{VRSketchIn}}: {{Exploring}} the {{Design Space}} of {{Pen}} and {{Tablet Interaction}} for {{3D Sketching}} in {{Virtual Reality}}},\n  shorttitle = {{{VRSketchIn}}},\n  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Drey, Tobias and Gugenheimer, Jan and Karlbauer, Julian and Milo, Maximilian and Rukzio, Enrico},\n  year = {2020},\n  month = apr,\n  series = {{{CHI}} '20},\n  pages = {1--14},\n  publisher = {Association for Computing Machinery},\n  address = {Honolulu, HI, USA},\n  doi = {10.1145/3313831.3376628},\n  abstract = {Sketching in virtual reality (VR) enhances perception and understanding of 3D volumes, but is currently a challenging task, as spatial input devices (e.g., tracked controllers) do not provide any scaffolding or constraints for mid-air interaction. We present VRSketchIn, a VR sketching application using a 6DoF-tracked pen and a 6DoF-tracked tablet as input devices, combining unconstrained 3D mid-air with constrained 2D surface-based sketching. To explore what possibilities arise from this combination of 2D (pen on tablet) and 3D input (6DoF pen), we present a set of design dimensions and define the design space for 2D and 3D sketching interaction metaphors in VR. We categorize prior art inside our design space and implemented a subset of metaphors for pen and tablet sketching in our prototype. To gain a deeper understanding which specific sketching operations users perform with 2D and which with 3D metaphors, we present findings of usability walkthroughs with six participants.},\n  isbn = {978-1-4503-6708-0}\n}",
            "DOI": "10.1145/3313831.3376628",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Remote Control",
                "Symmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Theory (-)"
            ],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Nuno Estalagem",
                "Augusto Esteves"
            ],
            "Name": "Between Wearable and Spatial Computing: Exploring Four Interaction Techniques at the Intersection of Smartwatches and Head-mounted Displays",
            "Bibtex": "@inproceedings{estalagem2024wearable,\n  title = {Between {{Wearable}} and {{Spatial Computing}}: {{Exploring Four Interaction Techniques}} at the {{Intersection}} of {{Smartwatches}} and {{Head-mounted Displays}}},\n  shorttitle = {Between {{Wearable}} and {{Spatial Computing}}},\n  booktitle = {Proceedings of the 2024 {{Symposium}} on {{Eye Tracking Research}} and {{Applications}}},\n  author = {Estalagem, Nuno and Esteves, Augusto},\n  year = {2024},\n  month = jun,\n  pages = {1--7},\n  publisher = {ACM},\n  address = {Glasgow United Kingdom},\n  doi = {10.1145/3649902.3656365},\n  isbn = {9798400706073},\n  langid = {english}\n}",
            "DOI": "10.1145/3649902.3656365",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Smartwatch"
            ],
            "Configuration": [
                "Remote Control",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Judith Friedl-Knirsch",
                "Christian Stach",
                "Christoph Anthes"
            ],
            "Name": "Exploring Collaboration for Data Analysis in Augmented Reality for Multiple Devices",
            "Bibtex": "@inproceedings{friedl-knirsch2023exploring,\n  title = {Exploring {{Collaboration}} for {{Data Analysis}} in {{Augmented Reality}} for {{Multiple Devices}}},\n  booktitle = {2023 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality Adjunct}} ({{ISMAR-Adjunct}})},\n  author = {{Friedl-Knirsch}, Judith and Stach, Christian and Anthes, Christoph},\n  year = {2023},\n  month = oct,\n  pages = {65--69},\n  publisher = {IEEE},\n  address = {Sydney, Australia},\n  doi = {10.1109/ISMAR-Adjunct60411.2023.00021},\n  isbn = {9798350328912}\n}",
            "DOI": "10.1109/ISMAR-Adjunct60411.2023.00021",
            "MR Devices": [
                "AR OST HWD",
                "Handheld AR (Tablet)",
                "AR VST HWD"
            ],
            "2D Devices": [
                "Laptop"
            ],
            "Configuration": [
                "Symmetric"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Multi-user - Indvidual Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Collaboration"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2018",
            "Authors": [
                "Chi-Jung Lee",
                "Hung-Kuo Chu"
            ],
            "Name": "Dual-MR: interaction with mixed reality using smartphones",
            "Bibtex": "@inproceedings{lee2018dualmr,\n  title = {Dual-{{MR}}: Interaction with Mixed Reality Using Smartphones},\n  shorttitle = {Dual-{{MR}}},\n  booktitle = {Proceedings of the 24th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},\n  author = {Lee, Chi-Jung and Chu, Hung-Kuo},\n  year = {2018},\n  month = nov,\n  series = {{{VRST}} '18},\n  pages = {1--2},\n  publisher = {Association for Computing Machinery},\n  address = {Tokyo, Japan},\n  doi = {10.1145/3281505.3281618},\n  abstract = {Mixed reality (MR) has changed the perspective we see and interact with our world. While the current-generation of MR head-mounted devices (HMDs) are capable of generating high quality visual contents, interation in most MR applications typically relies on in-air hand gestures, gaze, or voice. These interfaces although are intuitive to learn, may easily lead to inaccurate operations due to fatigue or constrained by the environment. In this work, we present Dual-MR, a novel MR interation system that i) synchronizes the MR viewpoints of HMD and handheld smartphone, and ii) enables precise, tactile, immersive and user-friendly object-level manipulations throught the multi-touch input of smartphone. In addition, Dual-MR allows multiple users to join the same MR coordinate system to facilite the collaborate in the same physical space, which further broadens its usability. A preliminary user study shows that our system easily overwhelms the conventional interface, which combines in-air hand gesture and gaze, in the completion time for a series of 3D object manipulation tasks in MR.},\n  isbn = {978-1-4503-6086-9}\n}",
            "DOI": "10.1145/3281505.3281618",
            "MR Devices": [
                "AR OST HWD",
                "Handheld AR (Smartphone)"
            ],
            "2D Devices": [],
            "Configuration": [
                "Symmetric"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User",
                "Multi-user - Indvidual Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Object Manipulation"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Gary Perelman",
                "Emmanuel Dubois",
                "Alice Probst",
                "Marcos Serrano"
            ],
            "Name": "Visual transitions around tabletops in mixed reality: study on a visual acquisition task between vertical virtual displays and horizontal tabletops",
            "Bibtex": "@article{perelman2022visual,\n  title = {Visual Transitions around Tabletops in Mixed Reality: Study on a Visual Acquisition Task between Vertical Virtual Displays and Horizontal Tabletops},\n  shorttitle = {Visual Transitions around Tabletops in Mixed Reality},\n  author = {Perelman, Gary and Dubois, Emmanuel and Probst, Alice and Serrano, Marcos},\n  year = {2022},\n  month = nov,\n  journal = {Proceedings of the ACM on Human-Computer Interaction},\n  volume = {6},\n  number = {ISS},\n  pages = {660--679},\n  issn = {2573-0142},\n  doi = {10.1145/3567738},\n  abstract = {See-through Head-Mounted Displays (HMDs) offer interesting opportunities to augment the interaction space around screens, especially around horizontal tabletops. In such context, HMDs can display surrounding vertical virtual windows to complement the tabletop content with data displayed in close vicinity. However, the effects of such combination on the visual acquisition of targets in the resulting combined display space have scarcely been explored. In this paper we conduct a study to explore visual acquisitions in such contexts, with a specific focus on the analysis of visual transitions between the horizontal tabletop display and the vertical virtual displays (in front and on the side of the tabletop). To further study the possible visual perception of the tabletop content out of the HMD and its impact on visual interaction, we distinguished two solutions for displaying information on the horizontal tabletop: using the see-through HMD to display virtual content over the tabletop surface (virtual overlay), i.e. the content is only visible inside the HMD's FoV, or using the tabletop itself (tabletop screen). 12 participants performed visual acquisition tasks involving the horizontal and vertical displays. We measured the time to perform the task, the head movements, the portions of the displays visible in the HMD's field of view, the physical fatigue and the user's preference. Our results show that it is faster to acquire virtual targets in the front display than on the side. Results reveal that the use of the virtual overlay on the tabletop slows down the visual acquisition compared to the use of the tabletop screen, showing that users exploit the visual perception of the tabletop content on the peripheral visual space. We were also able to quantify when and to which extent targets on the tabletop can be acquired without being visible within the HMD's field of view when using the tabletop screen, i.e. by looking under the HMD. These results lead to design recommendations for more efficient, comfortable and integrated interfaces combining tabletop and surrounding vertical virtual displays.},\n  langid = {english}\n}",
            "DOI": "10.1145/3567738",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "VESAD"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Theory (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Carole Plasson",
                "Renaud Blanch",
                "Laurence Nigay"
            ],
            "Name": "Selection Techniques for 3D Extended Desktop Workstation with AR HMD",
            "Bibtex": "@inproceedings{plasson2022selection,\n  title = {Selection {{Techniques}} for {{3D Extended Desktop Workstation}} with {{AR HMD}}},\n  booktitle = {2022 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},\n  author = {Plasson, Carole and Blanch, Renaud and Nigay, Laurence},\n  year = {2022},\n  month = oct,\n  pages = {460--469},\n  publisher = {IEEE},\n  address = {Singapore, Singapore},\n  doi = {10.1109/ISMAR55827.2022.00062},\n  copyright = {https://doi.org/10.15223/policy-029},\n  isbn = {978-1-66545-325-7}\n}",
            "DOI": "10.1109/ISMAR55827.2022.00062",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Mickael Sereno",
                "St\u00e9phane Gosset",
                "Lonni Besan\u00e7on",
                "Tobias Isenberg"
            ],
            "Name": "Hybrid Touch/Tangible Spatial Selection in Augmented Reality",
            "Bibtex": "@article{sereno2022hybrid,\n  title = {Hybrid {{Touch}}/{{Tangible Spatial Selection}} in {{Augmented Reality}}},\n  author = {Sereno, Mickael and Gosset, St{\\'e}phane and Besan{\\c c}on, Lonni and Isenberg, Tobias},\n  year = {2022},\n  month = jun,\n  journal = {Computer Graphics Forum},\n  volume = {41},\n  number = {3},\n  pages = {403},\n  doi = {10.1111/cgf.14550},\n  abstract = {We study tangible touch tablets combined with Augmented Reality Head-Mounted Displays (AR-HMDs) to perform spatial 3D selections. We are primarily interested in the exploration of 3D unstructured datasets such as cloud points or volumetric datasets. AR-HMDs immerse users by showing datasets stereoscopically, and tablets provide a set of 2D exploration tools. Because AR-HMDs merge the visualization, interaction, and the users' physical spaces, users can also use the tablets as tangible objects in their 3D space. Nonetheless, the tablets' touch displays provide their own visualization and interaction spaces, separated from those of the AR-HMD. This raises several research questions compared to traditional setups. In this paper, we theorize, discuss, and study different available mappings for manual spatial selections using a tangible tablet within an AR-HMD space. We then study the use of this tablet within a 3D AR environment, compared to its use with a 2D external screen.},\n  langid = {english}\n}",
            "DOI": "10.1111/cgf.14550",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Asymmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2015",
            "Authors": [
                "Marcos Serrano",
                "Barrett Ens",
                "Xing-Dong Yang",
                "Pourang Irani"
            ],
            "Name": "Gluey: Developing a Head-Worn Display Interface to Unify the Interaction Experience in Distributed Display Environments",
            "Bibtex": "@inproceedings{serrano2015gluey,\n  title = {Gluey: {{Developing}} a {{Head-Worn Display Interface}} to {{Unify}} the {{Interaction Experience}} in {{Distributed Display Environments}}},\n  shorttitle = {Gluey},\n  booktitle = {Proceedings of the 17th {{International Conference}} on {{Human-Computer Interaction}} with {{Mobile Devices}} and {{Services}}},\n  author = {Serrano, Marcos and Ens, Barrett and Yang, Xing-Dong and Irani, Pourang},\n  year = {2015},\n  month = aug,\n  series = {{{MobileHCI}} '15},\n  pages = {161--171},\n  publisher = {Association for Computing Machinery},\n  address = {Copenhagen, Denmark},\n  doi = {10.1145/2785830.2785838},\n  abstract = {Distributed display environments (DDEs) allow use of various specialized devices but challenge designers to provide a clean flow of data across multiple displays. Upcoming consumer-ready head-worn displays (HWDs) can play a central role in unifying the interaction experience in such ecosystems. In this paper, we report on the design and development of Gluey, a user interface that acts as a 'glue' to facilitate seamless input transitions and data movement across displays. Based on requirements we refine for such an interface, Gluey leverages inherent headworn display attributes such as field-of-view tracking and an always-available canvas to redirect input and migrate content across multiple displays, while minimizing device switching costs. We implemented a functional prototype integrating Gluey's numerous interaction possibilities. From our experience in this integration and from user evaluation results, we identify the open challenges in using HWDs to unify the interaction experience in DDEs.},\n  isbn = {978-1-4503-3652-9}\n}",
            "DOI": "10.1145/2785830.2785838",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Large Display",
                "Smartphone",
                "Tablet",
                "Laptop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "1997",
            "Authors": [
                "Ryugo Kijima",
                "Takeo Ojika"
            ],
            "Name": "Transition between virtual environment and workstation environment with projective head mounted display",
            "Bibtex": "@inproceedings{kijima1997transition,\n  title = {Transition between Virtual Environment and Workstation Environment with Projective Head Mounted Display},\n  booktitle = {Proceedings of {{IEEE}} 1997 {{Annual International Symposium}} on {{Virtual Reality}}},\n  author = {Kijima, R. and Ojika, T.},\n  year = {1997},\n  pages = {130--137},\n  publisher = {IEEE Comput. Soc. Press},\n  address = {Albuquerque, NM, USA},\n  doi = {10.1109/VRAIS.1997.583062},\n  isbn = {978-0-8186-7843-1}\n}",
            "DOI": "10.1109/VRAIS.1997.583062",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Logical Distribution",
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel",
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "David Aigner",
                "Nanjia Wang",
                "David Kielmayer",
                "J\u00fcrgen Steiner",
                "Julian Hochp\u00f6chler",
                "Christoph Heinzl",
                "Daniel Roth",
                "Frank Maurer",
                "Christoph Anthes"
            ],
            "Name": "Cardiac Visualisation Along the RV-Continuum - A High-Fidelity Pilot Study",
            "Bibtex": "@inproceedings{aigner2023cardiac,\n  title = {Cardiac {{Visualisation Along}} the {{RV-Continuum}} - {{A High-Fidelity Pilot Study}}},\n  booktitle = {2023 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality Adjunct}} ({{ISMAR-Adjunct}})},\n  author = {Aigner, David and Wang, Nanjia and Kielmayer, David and Steiner, J{\\\"u}rgen and Hochp{\\\"o}chler, Julian and Heinzl, Christoph and Roth, Daniel and Maurer, Frank and Anthes, Christoph},\n  year = {2023},\n  month = oct,\n  pages = {675--680},\n  issn = {2771-1110},\n  doi = {10.1109/ISMAR-Adjunct60411.2023.00145},\n  abstract = {The different stages along the Reality-Virtuality Continuum (RVC) come with individual challenges and benefits for visualisation. While visualisation at these stages has been explored extensively over the past decades, a combination including all stages is yet to be investigated. In this publication we introduce an early prototype from the area of cardiac surgery planning connecting the different RVC stages and allowing the user to move between real, augmented and virtual environment within a single application. An overview of the implemented concepts and the functionality is presented and a pilot study gathering data about the potential use of such a system for operation planning was executed.}\n}",
            "DOI": "10.1109/ISMAR-Adjunct60411.2023.00145",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2022",
            "Authors": [
                "Katja Krug",
                "Wolfgang B\u00fcschel",
                "Konstantin Klamka",
                "Raimund Dachselt"
            ],
            "Name": "CleAR Sight: Exploring the Potential of Interacting with Transparent Tablets in Augmented Reality",
            "Bibtex": "@inproceedings{krug2022clear,\n  title = {{{CleAR Sight}}: {{Exploring}} the {{Potential}} of {{Interacting}} with {{Transparent Tablets}} in {{Augmented Reality}}},\n  shorttitle = {{{CleAR Sight}}},\n  booktitle = {2022 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},\n  author = {Krug, Katja and B{\\\"u}schel, Wolfgang and Klamka, Konstantin and Dachselt, Raimund},\n  year = {2022},\n  month = oct,\n  pages = {196--205},\n  issn = {1554-7868},\n  doi = {10.1109/ISMAR55827.2022.00034},\n  abstract = {In this paper, we examine the potential of incorporating transparent, handheld devices into head-mounted Augmented Reality (AR). Additional mobile devices have long been successfully used in head-mounted AR, but they obscure the visual context and real world objects during interaction. Transparent tangible displays can address this problem, using either transparent OLED screens or rendering by the head-mounted display itself. However, so far, there is no systematic analysis of the use of such transparent tablets in combination with AR head-mounted displays (HMDs), with respect to their benefits and arising challenges. We address this gap by introducing a research platform based on a touch-enabled, transparent interaction panel, for which we present our custom hardware design and software stack in detail. Furthermore, we developed a series of interaction concepts for this platform and demonstrate them in the context of three use case scenarios: the exploration of 3D volumetric data, collaborative visual data analysis, and the control of smart home appliances. We validate the feasibility of our concepts with interactive prototypes that we used to elicit feedback from HCI experts. As a result, we contribute to a better understanding of how transparent tablets can be integrated into future AR environments.}\n}",
            "DOI": "10.1109/ISMAR55827.2022.00034",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Symmetric",
                "Dynamic Lens",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User",
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Text Entry / Annotations",
                "Other / None"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage",
                "Demonstration"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Daekun Kim",
                "Nikhita Joshi",
                "Daniel Vogel"
            ],
            "Name": "Perspective and Geometry Approaches to Mouse Cursor Control in Spatial Augmented Reality",
            "Bibtex": "@inproceedings{kim2023perspective,\n  title = {Perspective and {{Geometry Approaches}} to {{Mouse Cursor Control}} in {{Spatial Augmented Reality}}},\n  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Kim, Daekun and Joshi, Nikhita and Vogel, Daniel},\n  year = {2023},\n  month = apr,\n  series = {{{CHI}} '23},\n  pages = {1--19},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3544548.3580849},\n  abstract = {Spatial augmented reality (SAR) can extend desktop computing out of the monitor and into our surroundings, but extending the standard style of mouse input is challenging due to real-world geometry irregularity, gaps, and occlusion. We identify two general approaches for controlling a mouse cursor in SAR: perspective-based approaches based on raycasting, such as Nacenta et. al's Perspective Cursor, and geometry-based approaches that closely associate cursor movement with surface topology. For the latter, we introduce Everywhere Cursor, a geometry-based approach for indirect mouse cursor control for complex 3D surface geometry in SAR. A controlled experiment compares approaches. Results show the geometry-based Everywhere Cursor improves accuracy and precision by 29\\% to 60\\% on average in a tracing task, but when traversing long distances, the perspective-based Perspective Cursor and Raycasting techniques are 22\\% to 49\\% faster, albeit with 4\\% to 10\\% higher error rates.},\n  isbn = {978-1-4503-9421-5}\n}",
            "DOI": "10.1145/3544548.3580849",
            "MR Devices": [
                "Spatial AR (projector based)"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "VESAD",
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Theory"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2018",
            "Authors": [
                "Simon Butscher",
                "Sebastian Hubenschmid",
                "Jens M\u00fcller",
                "Johannes Fuchs",
                "Harald Reiterer"
            ],
            "Name": "Clusters, Trends, and Outliers: How Immersive Technologies Can Facilitate the Collaborative Analysis of Multidimensional Data",
            "Bibtex": "@inproceedings{butscher2018clusters,\n  title = {Clusters, {{Trends}}, and {{Outliers}}: {{How Immersive Technologies Can Facilitate}} the {{Collaborative Analysis}} of {{Multidimensional Data}}},\n  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '18},\n  author = {Butscher, Simon and Hubenschmid, Sebastian and M{\\\"u}ller, Jens and Fuchs, Johannes and Reiterer, Harald},\n  year = {2018},\n  pages = {1--12},\n  publisher = {ACM Press},\n  address = {New York},\n  issn = {03276139 (ISSN)},\n  doi = {10.1145/3173574.3173664},\n  abstract = {Immersive technologies such as augmented reality devices are opening up a new design space for the visual analysis of data. This paper studies the potential of an augmented reality environment for the purpose of collaborative analysis of multi- dimensional, abstract data. We present ART, a collaborative analysis tool to visualize multidimensional data in augmented reality using an interactive, 3D parallel coordinates visual- ization. The visualization is anchored to a touch-sensitive tabletop, benefiting from well-established interaction tech- niques. The results of group-based expert walkthroughs show that ART can facilitate immersion in the data, a fluid analysis process, and collaboration. Based on the results, we provide a set of guidelines and discuss future research areas to foster the development of immersive technologies as tools for the collaborative analysis of multidimensional data.},\n  isbn = {978-1-4503-5620-6}\n}",
            "DOI": "10.1145/3173574.3173664",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "Symmetric",
                "Logical Distribution",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Weizhou Luo",
                "Eva Goebel",
                "Patrick Reipschl\u00e4ger",
                "Mats Ellenberg Ole",
                "Raimund Dachselt"
            ],
            "Name": "Exploring and Slicing Volumetric Medical Data in Augmented Reality Using a Spatially-Aware Mobile Device",
            "Bibtex": "@inproceedings{luo2021exploringa,\n  title = {Exploring and {{Slicing Volumetric Medical Data}} in {{Augmented Reality Using}} a {{Spatially-Aware Mobile Device}}},\n  booktitle = {2021 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality Adjunct}} ({{ISMAR-Adjunct}})},\n  author = {Luo, Weizhou and Goebel, Eva and Reipschl{\\\"a}ger, Patrick and Ellenberg, Mats Ole and Dachselt, Raimund},\n  year = {2021},\n  month = oct,\n  pages = {334--339},\n  doi = {10.1109/ISMAR-Adjunct54149.2021.00076},\n  abstract = {We present a concept and early prototype for exploring volumetric medical data, e.g., from MRI or CT scans, in head-mounted Augmented Reality (AR) with a spatially tracked tablet. Our goal is to address the lack of immersion and intuitive input of conventional systems by providing spatial navigation to extract arbitrary slices from volumetric data directly in three-dimensional space. A 3D model of the medical data is displayed in the real environment, fixed to a particular location, using AR. The tablet is spatially moved through this virtual 3D model and shows the resulting slices as 2D images. We present several techniques that facilitate this overall concept, e.g., to place and explore the model, as well as to capture, annotate, and compare slices of the data. Furthermore, we implemented a proof-of-concept prototype that demonstrates the feasibility of our concepts. With our work we want to improve the current way of working with volumetric data slices in the medical domain and beyond.}\n}",
            "DOI": "10.1109/ISMAR-Adjunct54149.2021.00076",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Medical",
                "SciVis"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Theory (-)"
            ],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "1997",
            "Authors": [
                "Zsolt Szalav\u00e1ri",
                "Michael Gervautz"
            ],
            "Name": "The Personal Interaction Panel \u2013 a Two-Handed Interface for Augmented Reality",
            "Bibtex": "@article{szalavari1997personala,\n  title = {The {{Personal Interaction Panel}} -- a {{Two-Handed Interface}} for {{Augmented Reality}}},\n  author = {Szalav{\\'a}ri, Zsolt and Gervautz, Michael},\n  year = {1997},\n  journal = {Computer Graphics Forum},\n  volume = {16},\n  number = {3},\n  pages = {C335-C346},\n  issn = {1467-8659},\n  doi = {10.1111/1467-8659.00137},\n  abstract = {This paper describes the introduction of a new interaction paradigm to augmented reality applications. The everyday tool handling experience of working with pen and notebooks is extended to create a three dimensional two-handed interface, that supports easy-to-understand manipulation tasks in augmented and virtual environments. In the design step we take advantage from the freedom, given by our very low demands on hardware and augment form and functionality to this device. On the basis of examples from object manipulation, augmented research environments and scientific visualization we show the generality of applicability. Although being in the first stages implementation, we consider the wide spectrum of suitability for different purposes.},\n  langid = {english}\n}",
            "DOI": "10.1111/1467-8659.00137",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Asymmetric",
                "Logical Distribution",
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "SciVis"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Shalva Kohen",
                "Carmine Elvezio",
                "Steven Feiner"
            ],
            "Name": "MiXR: A Hybrid AR Sheet Music Interface for Live Performance",
            "Bibtex": "@inproceedings{kohen2020mixr,\n  title = {{{MiXR}}: {{A Hybrid AR Sheet Music Interface}} for {{Live Performance}}},\n  shorttitle = {{{MiXR}}},\n  booktitle = {2020 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality Adjunct}} ({{ISMAR-Adjunct}})},\n  author = {Kohen, Shalva and Elvezio, Carmine and Feiner, Steven},\n  year = {2020},\n  month = nov,\n  pages = {76--77},\n  doi = {10.1109/ISMAR-Adjunct51615.2020.00035},\n  abstract = {Musicians face a number of issues when performing live, including organizing and annotating sheet music. This can be an unwieldy process, as musicians need to simultaneously read and manipulate sheet music and interact with the conductor and other musicians. Augmented Reality can provide a way to ease some of the more cumbersome aspects of live performance and practice. We present MiXR, a novel interactive system that combines an AR headset, a smartphone, and a tablet to allow performers to intuitively and efficiently manage and annotate virtual sheet music in their physical environment. We discuss our underlying motivation, the interaction techniques supported, and the system architecture.}\n}",
            "DOI": "10.1109/ISMAR-Adjunct51615.2020.00035",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet",
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Entertainment"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2007",
            "Authors": [
                "Hrvoje Benko",
                "Steven Feiner"
            ],
            "Name": "Balloon Selection: A Multi-Finger Technique for Accurate Low-Fatigue 3D Selection",
            "Bibtex": "@inproceedings{benko2007balloon,\n  title = {Balloon {{Selection}}: {{A Multi-Finger Technique}} for {{Accurate Low-Fatigue 3D Selection}}},\n  shorttitle = {Balloon {{Selection}}},\n  booktitle = {2007 {{IEEE Symposium}} on {{3D User Interfaces}}},\n  author = {Benko, Hrvoje and Feiner, Steven},\n  year = {2007},\n  month = mar,\n  doi = {10.1109/3DUI.2007.340778},\n  abstract = {Balloon selection is a 3D interaction technique that is modeled after the real world metaphor of manipulating a helium balloon attached to a string. Balloon selection allows for precise 3D selection in the volume above a tabletop surface by using multiple fingers on a multi-touch-sensitive surface. The 3DOF selection tasks is decomposed in part into a 2DOF positioning task performed by one finger on the tabletop in an absolute 2D Cartesian coordinate system and a 1DOF positioning task performed by another finger on the tabletop in a relative 2D polar coordinate system. We have evaluated balloon selection in a formal user study that compared it to two well-known interaction techniques for selecting a static 3D target: a 3DOF tracked wand and keyboard cursor keys. We found that balloon selection was significantly faster than using cursor keys and had a significantly lower error rate than the wand. The lower error rate appeared to result from the user's hands being supported by the tabletop surface, resulting in significantly reduced hand tremor and arm fatigue.}\n}",
            "DOI": "10.1109/3DUI.2007.340778",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2004",
            "Authors": [
                "Hrvoje Benko",
                "Edward W. Ishak",
                "Steven Feiner"
            ],
            "Name": "Collaborative mixed reality visualization of an archaeological excavation",
            "Bibtex": "@inproceedings{benko2004collaborativea,\n  title = {Collaborative Mixed Reality Visualization of an Archaeological Excavation},\n  booktitle = {Third {{IEEE}} and {{ACM International Symposium}} on {{Mixed}} and {{Augmented Reality}}},\n  author = {Benko, H. and Ishak, E.W. and Feiner, S.},\n  year = {2004},\n  month = nov,\n  pages = {132--140},\n  doi = {10.1109/ISMAR.2004.23},\n  abstract = {We present VITA (visual interaction tool for archaeology), an experimental collaborative mixed reality system for offsite visualization of an archaeological dig. Our system allows multiple users to visualize the dig site in a mixed reality environment in which tracked, see-through, head-worn displays are combined with a multi-user, multi-touch, projected table surface, a large screen display, and tracked hand-held displays. We focus on augmenting existing archaeological analysis methods with new ways to organize, visualize, and combine the standard 2D information available from an excavation (drawings, pictures, and notes) with textured, laser range-scanned 3D models of objects and the site itself. Users can combine speech, touch, and 3D hand gestures to interact multimodally with the environment. Preliminary user tests were conducted with archaeology researchers and students, and their feedback is presented here.}\n}",
            "DOI": "10.1109/ISMAR.2004.23",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tabletop",
                "Large Display"
            ],
            "Configuration": [
                "Asymmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Other Terms",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2019",
            "Authors": [
                "Hemant Surale Bhaskar",
                "Aakar Gupta",
                "Mark Hancock",
                "Daniel Vogel"
            ],
            "Name": "TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality",
            "Bibtex": "@inproceedings{surale2019tabletinvr,\n  title = {{{TabletInVR}}: {{Exploring}} the {{Design Space}} for {{Using}} a {{Multi-Touch Tablet}} in {{Virtual Reality}}},\n  shorttitle = {{{TabletInVR}}},\n  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},\n  author = {Surale, Hemant Bhaskar and Gupta, Aakar and Hancock, Mark and Vogel, Daniel},\n  year = {2019},\n  month = may,\n  series = {{{CHI}} '19},\n  pages = {1--13},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3290605.3300243},\n  abstract = {Complex virtual reality (VR) tasks, like 3D solid modelling, are challenging with standard input controllers. We propose exploiting the affordances and input capabilities when using a 3D-tracked multi-touch tablet in an immersive VR environment. Observations gained during semi-structured interviews with general users, and those experienced with 3D software, are used to define a set of design dimensions and guidelines. These are used to develop a vocabulary of interaction techniques to demonstrate how a tablet's precise touch input capability, physical shape, metaphorical associations, and natural compatibility with barehand mid-air input can be used in VR. For example, transforming objects with touch input, \"cutting\" objects by using the tablet as a physical \"knife\", navigating in 3D by using the tablet as a viewport, and triggering commands by interleaving bare-hand input around the tablet. Key aspects of the vocabulary are evaluated with users, with results validating the approach.},\n  isbn = {978-1-4503-5970-2}\n}",
            "DOI": "10.1145/3290605.3300243",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Theory"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Pascal Knierim",
                "Dimitri Hein",
                "Albrecht Schmidt",
                "Thomas Kosch"
            ],
            "Name": "The SmARtphone Controller: Leveraging Smartphones as Input and Output Modality for Improved Interaction within Mobile Augmented Reality Environments",
            "Bibtex": "@article{knierim2021smartphone,\n  title = {The {{SmARtphone Controller}}: {{Leveraging Smartphones}} as {{Input}} and {{Output Modality}} for {{Improved Interaction}} within {{Mobile Augmented Reality Environments}}},\n  shorttitle = {The {{SmARtphone Controller}}},\n  author = {Knierim, Pascal and Hein, Dimitri and Schmidt, Albrecht and Kosch, Thomas},\n  year = {2021},\n  month = apr,\n  journal = {i-com},\n  volume = {20},\n  number = {1},\n  pages = {49--61},\n  publisher = {Oldenbourg Wissenschaftsverlag},\n  issn = {2196-6826},\n  doi = {10.1515/icom-2021-0003},\n  abstract = {Current interaction modalities for mobile Augmented Reality (AR) are tedious and lack expressiveness. To overcome these prevalent limitations, we developed and evaluated a multimodal interaction concept by pairing a smartphone as an input and output modality for mobile AR. In a user study (n = 24), we investigated the effects on interaction speed, accuracy, and task load for (1) virtual object manipulation as well as (2) interaction with established graphical user interfaces (GUIs). Our findings show that a smartphone-based AR controller results in significantly faster and more accurate object manipulation with reduced task load than state-of-art mid-air gestures. Our results also indicate a significant enhancement for using the physical touchscreen as a modality compared to mid-air gestures for GUI interaction. We conclude that interaction in mobile AR environments can be improved by utilizing a smartphone as an omnipresent controller. Additionally, we discuss how future AR systems can benefit from tangible touchscreens as an additional and complementary interaction modality.},\n  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},\n  langid = {english}\n}",
            "DOI": "10.1515/icom-2021-0003",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Productivity"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Other Terms"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Julius von Willich",
                "Frank Nelles",
                "Wen-Jie Tseng",
                "Jan Gugenheimer",
                "Sebastian G\u00fcnther",
                "Max M\u00fchlh\u00e4user"
            ],
            "Name": "A Qualitative Investigation of User Transitions and Frictions in Cross-Reality Applications",
            "Bibtex": "@inproceedings{10.1145/3706598.3713921,\n  author = {von Willich, Julius and Nelles, Frank and Tseng, Wen-Jie and Gugenheimer, Jan and G\\\"{u}nther, Sebastian and M\\\"{u}hlh\\\"{a}user, Max},\n  title = {A Qualitative Investigation of User Transitions and Frictions in Cross-Reality Applications},\n  year = {2025},\n  isbn = {9798400713941},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3706598.3713921},\n  doi = {10.1145/3706598.3713921},\n  abstract = {Research in Augmented Reality (AR) and Virtual Reality (VR) has mostly viewed them in isolation. Yet, when used together in practical settings, AR and VR each offer unique strengths, necessitating multiple transitions to harness their advantages. This paper investigates potential challenges in Cross-Reality (CR) transitions to inform future application design. We implemented a CR system featuring a 3D modeling task that requires users to switch between PC, AR, and VR. Using a talk-aloud study (n=12) and thematic analysis, we revealed that frictions primarily arose when transitions conflicted with users\u2019 Spatial Mental Model (SMM). Furthermore, we found five transition archetypes employed to enhance productivity once an SMM was established. Our findings uncover that transitions have to focus on establishing and upholding the SMM of users across realities, by communicating differences between them.},\n  booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},\n  articleno = {808},\n  numpages = {18},\n  keywords = {Cross-Reality Transitions, Augmented Reality, Virtual Reality, Cross-Device Interaction, Transitional Interfaces},\n  location = {},\n  series = {CHI '25}\n}",
            "DOI": "10.1145/3706598.3713921",
            "MR Devices": [
                "AR VST HWD",
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Transitional",
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Jeremy McDade",
                "Allison Jing",
                "Andrew Cunningham"
            ],
            "Name": "RealityDrop: A Framework Using Multimodal Superhuman Techniques to Manipulate Cross-Reality Virtual Content",
            "Bibtex": "@inproceedings{10.1145/3746058.3758429,\n   author = {McDade, Jeremy and Jing, Allison and Cunningham, Andrew},\n   title = {RealityDrop: A Framework Using Multimodal Superhuman Techniques to Manipulate Cross-Reality Virtual Content},\n   year = {2025},\n   isbn = {9798400720369},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3746058.3758429},\n   doi = {10.1145/3746058.3758429},\n   abstract = {We present RealityDrop, a novel framework using the \u201csuperhuman\" multimodal interaction techniques to transfer, manipulate, and display cross-reality virtual content among domain experts. Employing Mixed Reality as the centre of control, RealityDrop affords a diverse background of multiusers (expert vs novice) to disseminate digital content (overview vs constituents) at varied control points to display space (near vs far) through multimodal superhuman interaction techniques (free hand vs hand+gaze). It builds an easy cross-system content transfer workflow that no longer requires the configuration and alignment of each display and environment. 1 customised content interpreter, 3 multimodal interaction techniques, and 2 cross-system transfer interfaces are incorporated for fluent content manipulation and presentation during collaboration.},\n   booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},\n   articleno = {110},\n   numpages = {3},\n   location = {},\n   series = {UIST Adjunct '25}\n}",
            "DOI": "10.1145/3746058.3758429",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Vera Marie Memmesheimer",
                "Kai Jonas Klingshirn",
                "Cindy Herold",
                "Bahram Ravani",
                "Achim Ebert"
            ],
            "Name": "Move'n'Hold Pro: Consistent Spatial Interaction Techniques for Object Manipulation with Handheld and Head-mounted Displays in Extended Reality",
            "Bibtex": "@inproceedings{10.1145/3673805.3673814,\n   author = {Memmesheimer, Vera Marie and Klingshirn, Kai Jonas and Herold, Cindy and Ravani, Bahram and Ebert, Achim},\n   title = {Move'n'Hold Pro: Consistent Spatial Interaction Techniques for Object Manipulation with Handheld and Head-mounted Displays in Extended Reality},\n   year = {2024},\n   isbn = {9798400718243},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3673805.3673814},\n   doi = {10.1145/3673805.3673814},\n   abstract = {Extended Reality (XR) technologies are still lacking appropriate interaction methods that enable users to seamlessly switch between different XR devices and degrees of virtuality. Addressing this gap, we present Move\u2019n\u2019Hold Pro \u2013 a set of consistent object manipulation techniques that are available for Mixed Reality handheld displays (MR-HHDs) as well as for Mixed and Virtual Reality head-mounted displays (MR-/VR-HMDs). Move\u2019n\u2019Hold Pro extends MR- and VR-HMDs with a tablet controller that implements object manipulation methods proposed by latest research on MR-HHD-UIs. Thereby, users can combine tablet movement and peripheral touch to translate or rotate virtual objects through direct or continuous manipulations. In our evaluation, comparing Move\u2019n\u2019Hold Pro to a State of the Art system, Move\u2019n\u2019Hold Pro was rated as the preferred system and to be easier to relearn. Furthermore, Move\u2019n\u2019Hold Pro reduced cognitive efforts, improved usability, and provided more cross-device benefits.},\n   booktitle = {Proceedings of the European Conference on Cognitive Ergonomics 2024},\n   articleno = {10},\n   numpages = {8},\n   keywords = {Augmented Reality, Extended Reality, Handheld displays, Head-mounted Displays, Interaction Technique, Mixed Reality, Spatial Interaction, User Interface, Virtual Reality},\n   location = {Paris, France},\n   series = {ECCE '24}\n}",
            "DOI": "10.1145/3673805.3673814",
            "MR Devices": [
                "AR OST HWD",
                "Handheld AR (Tablet)",
                "VR HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Study"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Sunyoung Bang",
                "Hyunjin Lee",
                "Seo Young Oh",
                "Woontack Woo"
            ],
            "Name": "AReading with Smartphones: Understanding the Trade-offs between Enhanced Legibility and Display Switching Costs in Hybrid AR Interfaces",
            "Bibtex": "@inproceedings{10.1145/3706598.3713879,\n   author = {Bang, Sunyoung and Lee, Hyunjin and Oh, Seo Young and Woo, Woontack},\n   title = {AReading with Smartphones: Understanding the Trade-offs between Enhanced Legibility and Display Switching Costs in Hybrid AR Interfaces},\n   year = {2025},\n   isbn = {9798400713941},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3706598.3713879},\n   doi = {10.1145/3706598.3713879},\n   abstract = {This research investigates the use of hybrid user interfaces to enhance text readability in augmented reality (AR) by combining optical see-through head-mounted displays with smartphones. While this integration can improve information legibility, it may also introduce display switching side effects. The extent to which these side effects hinder user experience and when the benefits outweigh drawbacks remain unclear. To address this gap, we conducted an empirical study (N=24) to evaluate how hybrid user interfaces affect AR reading tasks across different content distances, which induce varying levels of display switching. Our findings show that hybrid user interfaces offer significant readability benefits compared to using the HMD only, reducing mental and physical demands when reading text linked to content at closer distances. However, as the distance between displays increases, the compensatory behaviors users adopt to manage increased switching costs negate these benefits, making hybrid user interfaces less effective. Based on these findings, we suggest (1) using smartphones as supplementary displays for text in reading-intensive tasks, (2) implementing adaptive display positioning to minimize switching overhead in such scenarios, and (3) adjusting the smartphone\u2019s role based on content distance for less intensive reading tasks. These insights provide guidance for optimizing smartphone integration in hybrid interfaces and enhancing AR systems for reading applications.},\n   booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},\n   articleno = {1231},\n   numpages = {20},\n   keywords = {hybrid user interfaces, OST HMD, readability, attention switching, augmented reality},\n   location = {},\n   series = {CHI '25}\n}",
            "DOI": "10.1145/3706598.3713879",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Tobias Rau",
                "Tobias Isenberg",
                "Andreas Koehn",
                "Michael Sedlmair",
                "Benjamin Lee"
            ],
            "Name": "Traversing Dual Realities: Investigating Techniques for Transitioning 3D Objects between Desktop and Augmented Reality Environments",
            "Bibtex": "@inproceedings{10.1145/3706598.3713949,\n   author = {Rau, Tobias and Isenberg, Tobias and Koehn, Andreas and Sedlmair, Michael and Lee, Benjamin},\n   title = {Traversing Dual Realities: Investigating Techniques for Transitioning 3D Objects between Desktop and Augmented Reality Environments},\n   year = {2025},\n   isbn = {9798400713941},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3706598.3713949},\n   doi = {10.1145/3706598.3713949},\n   abstract = {Desktop environments can integrate augmented reality (AR) head-worn devices to support 3D representations, visualizations, and interactions in a novel yet familiar setting. As users navigate across the dual realities\u2014desktop and AR\u2014a way to move 3D objects between them is needed. We devise three baseline transition techniques based on common approaches in the literature and evaluate their usability and practicality in an initial user study (N=18). After refining both our transition techniques and the surrounding technical setup, we validate the applicability of the overall concept for real-world activities in an expert user study (N=6). In it, computational chemists followed their usual desktop workflows to build, manipulate, and analyze 3D molecular structures, but now aided with the addition of AR and our transition techniques. Based on our findings from both user studies, we provide lessons learned and takeaways for the design of 3D object transition techniques in desktop + AR environments.},\n   booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},\n   articleno = {1236},\n   numpages = {16},\n   keywords = {Augmented reality, Cross-reality, Hybrid user interfaces, Usability study, Expert study, Computational Chemistry, Gestural input},\n   location = {},\n   series = {CHI '25}\n}",
            "DOI": "10.1145/3706598.3713949",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Reality",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Leping Qiu",
                "Erin Seongyoon Kim",
                "Sangho Suh",
                "Ludwig Sidenmark",
                "Tovi Grossman"
            ],
            "Name": "MaRginalia: Enabling In-person Lecture Capturing and Note-taking Through Mixed Reality",
            "Bibtex": "@inproceedings{10.1145/3706598.3714065,\n   author = {Qiu, Leping and Kim, Erin Seongyoon and Suh, Sangho and Sidenmark, Ludwig and Grossman, Tovi},\n   title = {MaRginalia: Enabling In-person Lecture Capturing and Note-taking Through Mixed Reality},\n   year = {2025},\n   isbn = {9798400713941},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3706598.3714065},\n   doi = {10.1145/3706598.3714065},\n   abstract = {Students often take digital notes during live lectures, but current methods can be slow when capturing information from lecture slides or the instructor\u2019s speech, and require them to focus on their devices, leading to distractions and missing important details. This paper explores supporting live lecture note-taking with mixed reality (MR) to quickly capture lecture information and take notes while staying engaged with the lecture. A survey and interviews with university students revealed common note-taking behaviors and challenges to inform the design. We present MaRginalia to provide digital note-taking with a stylus tablet and MR headset. Students can take notes with an MR representation of the tablet, lecture slides, and audio transcript without looking down at their device. When preferred, students can also perform detailed interactions by looking at the physical tablet. We demonstrate the feasibility and usefulness of MaRginalia and MR-based note-taking in a user study with 12 students.},\n   booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},\n   articleno = {141},\n   numpages = {15},\n   keywords = {Note-taking, Cross-device Interaction, Mixed-reality system, Pen-based Input},\n   location = {},\n   series = {CHI '25}\n}",
            "DOI": "10.1145/3706598.3714065",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Remote Control",
                "Symmetric",
                "Logical Distribution",
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Text Entry / Annotations",
                "Productivity"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Keming Chen",
                "Qihao Yang",
                "Qingshu Yuan",
                "Jin Xu",
                "Zhengwei Yao",
                "Zhigeng Pan"
            ],
            "Name": "CoPadSAR: A Spatial Augmented Reality Interaction Approach for Collaborative Design via Pad-Based Cross-Device Interaction",
            "Bibtex": "@article{10.1002/cav.70065,\n   author = {Chen, Keming and Yang, Qihao and Yuan, Qingshu and Xu, Jin and Yao, Zhengwei and Pan, Zhigeng},\n   title = {CoPadSAR: A Spatial Augmented Reality Interaction Approach for Collaborative Design via Pad-Based Cross-Device Interaction},\n   journal = {Computer Animation and Virtual Worlds},\n   volume = {36},\n   number = {4},\n   pages = {e70065},\n   keywords = {collaboration, computer-supported collaborative design, cross-device interaction, spatial augmented reality},\n   doi = {https://doi.org/10.1002/cav.70065},\n   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.70065},\n   eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cav.70065},\n   abstract = {Augmented reality (AR) is a technology that superimposes digital information onto the real world. As one of the three major forms of AR, spatial augmented reality (SAR) projects virtual content into public spaces, making it accessible to collaborators. Due to its shared large display area, SAR has significant potential for application in collaborative design. However, existing SAR interaction methods may suffer from inefficiencies and poor collaborative experiences. To address this issue, CoPadSAR, a Pad-based cross-device interaction method, is proposed. It can map 2D operations from each Pad onto 3D objects within the SAR environment, allowing users to collaborate using multiple Pads. Moreover, a prototype is presented that supports collaborative painting, annotation, and object creation. Furthermore, a comparative study involving 40 participants (20 pairs) is conducted. The results indicate CoPadSAR reveals better group performance than controller-based, gesture, and tangible interactions. It has greater usability and provides a better collaborative experience. The interviews further confirm the user preference for it. This study contributes to expanding the application of SAR in collaborative design.},\n   year = {2025}\n}",
            "DOI": "10.1002/cav.70065",
            "MR Devices": [
                "Spatial AR (projector based)",
                "Handheld AR (Tablet)"
            ],
            "2D Devices": [],
            "Configuration": [
                "Symmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Robbe Cools",
                "Inne Maerevoet",
                "Matt Gottsacker",
                "Adalberto L. Simeone"
            ],
            "Name": "Comparison of Cross-Reality Transition Techniques Between 3D and 2D Display Spaces in Desktop-AR Systems",
            "Bibtex": "@ARTICLE{10919011,\n  author={Cools, Robbe and Maerevoet, Inne and Gottsacker, Matt and Simeone, Adalberto L.},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Comparison of Cross-Reality Transition Techniques Between 3D and 2D Display Spaces in Desktop-AR Systems}, \n  year={2025},\n  volume={31},\n  number={5},\n  pages={2798-2805},\n  keywords={Three-dimensional displays;Two-dimensional displays;Switches;Resists;Hands;Aerospace electronics;Monitoring;Electronic mail;Input devices;Color;Cross-Reality;Hybrid User Interface;Desktop-AR;Transition},\n  doi={10.1109/TVCG.2025.3549907}\n}",
            "DOI": "10.1109/TVCG.2025.3549907",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Reality",
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Lixiang Zhao",
                "Tobias Isenberg",
                "Fuqi Xie",
                "Hai-Ning Liang",
                "Lingyun Yu"
            ],
            "Name": "SpatialTouch: Exploring Spatial Data Visualizations in Cross-Reality",
            "Bibtex": "@article{10.1109/TVCG.2024.3456368,\n   author = {Zhao, Lixiang and Isenberg, Tobias and Xie, Fuqi and Liang, Hai-Ning and Yu, Lingyun},\n   title = {SpatialTouch: Exploring Spatial Data Visualizations in Cross-Reality},\n   year = {2025},\n   issue_date = {Jan. 2025},\n   publisher = {IEEE Educational Activities Department},\n   address = {USA},\n   volume = {31},\n   number = {1},\n   issn = {1077-2626},\n   url = {https://doi.org/10.1109/TVCG.2024.3456368},\n   doi = {10.1109/TVCG.2024.3456368},\n   abstract = {We propose and study a novel cross-reality environment that seamlessly integrates a monoscopic 2D surface (an interactive screen with touch and pen input) with a stereoscopic 3D space (an augmented reality HMD) to jointly host spatial data visualizations. This innovative approach combines the best of two conventional methods of displaying and manipulating spatial 3D data, enabling users to fluidly explore diverse visual forms using tailored interaction techniques. Providing such effective 3D data exploration techniques is pivotal for conveying its intricate spatial structures\u2014often at multiple spatial or semantic scales\u2014across various application domains and requiring diverse visual representations for effective visualization. To understand user reactions to our new environment, we began with an elicitation user study, in which we captured their responses and interactions. We observed that users adapted their interaction approaches based on perceived visual representations, with natural transitions in spatial awareness and actions while navigating across the physical surface. Our findings then informed the development of a design space for spatial data exploration in cross-reality. We thus developed cross-reality environments tailored to three distinct domains: for 3D molecular structure data, for 3D point cloud data, and for 3D anatomical data. In particular, we designed interaction techniques that account for the inherent features of interactions in both spaces, facilitating various forms of interaction, including mid-air gestures, touch interactions, pen interactions, and combinations thereof, to enhance the users' sense of presence and engagement. We assessed the usability of our environment with biologists, focusing on its use for domain research. In addition, we evaluated our interaction transition designs with virtual and mixed-reality experts to gather further insights. As a result, we provide our design suggestions for the cross-reality environment, emphasizing the interaction with diverse visual representations and seamless interaction transitions between 2D and 3D spaces.},\n   journal = {IEEE Transactions on Visualization and Computer Graphics},\n   month = jan,\n   pages = {897\u2013907},\n   numpages = {11}\n}",
            "DOI": "10.1109/TVCG.2024.3456368",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Tabletop"
            ],
            "Configuration": [
                "Asymmetric",
                "Migratory Interface",
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "SciVis"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Wai Tong",
                "Haobo Li",
                "Meng Xia",
                "Wong Kam-Kwai",
                "Ting-Chuen Pong",
                "Huamin Qu"
            ],
            "Name": "Exploring Spatial Hybrid User Interface for Visual Sensemaking",
            "Bibtex": "@ARTICLE{10870571,\n  author={Tong, Wai and Li, Haobo and Xia, Meng and Kam-Kwai, Wong and Pong, Ting-Chuen and Qu, Huamin and Yang, Yalong},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Exploring Spatial Hybrid User Interface for Visual Sensemaking}, \n  year={2025},\n  volume={31},\n  number={10},\n  pages={7062-7077},\n  keywords={Data visualization;Visualization;Three-dimensional displays;Switches;Navigation;Keyboards;Hands;User experience;Rendering (computer graphics);Visual analytics;Hybrid user interface;data visualization;node-link diagram;visual sensemaking;document analysis},\n  doi={10.1109/TVCG.2025.3538771}\n}",
            "DOI": "10.1109/TVCG.2025.3538771",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Asymmetric",
                "Migratory Interface"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal",
                "Social"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Study"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Brody Wells",
                "Nanjia Wang",
                "Samuel Wiebe",
                "Colin B Josephson",
                "Farnaz Sinaei",
                "Frank Maurer"
            ],
            "Name": "NeuroCR: Integrating Mixed Reality with Desktop Medical Imaging Software for Pre-surgical Evaluations of Epilepsy Patients",
            "Bibtex": "@INPROCEEDINGS{10972704,\n  author={Wells, Brody and Wang, Nanjia and Wiebe, Samuel and Josephson, Colin B and Sinaei, Farnaz and Maurer, Frank},\n  booktitle={2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, \n  title={NeuroCR: Integrating Mixed Reality with Desktop Medical Imaging Software for Pre-surgical Evaluations of Epilepsy Patients}, \n  year={2025},\n  volume={},\n  number={},\n  pages={1236-1237},\n  keywords={Hands;Three-dimensional displays;Extended reality;Prototypes;Data visualization;Mixed reality;Epilepsy;Software;Usability;Biomedical imaging;Serious eXtended Reality;Mixed Reality;Cross Reality;Visualization;Visualization Techniques},\n  doi={10.1109/VRW66409.2025.00264}\n}",
            "DOI": "10.1109/VRW66409.2025.00264",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Asymmetric"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Medical"
            ],
            "Terminology": [
                "Term: Cross-Reality"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Shintaro Imatani",
                "Kensuke Tobitani",
                "Kyo Akabane"
            ],
            "Name": "TouchWIM: Object Manipulation in AR Spatial Design With World in Miniature and Hybrid User Interface",
            "Bibtex": "@ARTICLE{10969634,\n  author={Imatani, Shintaro and Tobitani, Kensuke and Akabane, Kyo},\n  journal={IEEE Access}, \n  title={TouchWIM: Object Manipulation in AR Spatial Design With World in Miniature and Hybrid User Interface}, \n  year={2025},\n  volume={13},\n  number={},\n  pages={69269-69280},\n  keywords={User interfaces;Three-dimensional displays;Smart phones;Virtual environments;Resists;Hands;Layout;Input devices;Solid modeling;Data visualization;Augmented reality;mixed reality;human-computer interaction;user interface},\n  doi={10.1109/ACCESS.2025.3562253}\n}",
            "DOI": "10.1109/ACCESS.2025.3562253",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Study"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Jan Krieglstein",
                "Jan Kolberg",
                "Aim\u00e9e Sousa Calepso",
                "Werner Kraus",
                "Michael Sedlmair"
            ],
            "Name": "A Hybrid User Interface Combining AR, Desktop, and Mobile Interfaces for Enhanced Industrial Robot Programming",
            "Bibtex": "@INPROCEEDINGS{11128291,\n  author={Krieglstein, Jan and Kolberg, Jan and Calepso, Aim\u00e9e Sousa and Kraus, Werner and Sedlmair, Michael},\n  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, \n  title={A Hybrid User Interface Combining AR, Desktop, and Mobile Interfaces for Enhanced Industrial Robot Programming}, \n  year={2025},\n  volume={},\n  number={},\n  pages={4738-4745},\n  keywords={Visualization;Three-dimensional displays;Two-dimensional displays;Resists;User interfaces;Industrial robots;User experience;Trajectory;Usability;Robot programming},\n  doi={10.1109/ICRA55743.2025.11128291}\n}",
            "DOI": "10.1109/ICRA55743.2025.11128291",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Laptop",
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution",
                "Migratory Interface"
            ],
            "Temporal": [
                "Exclusive",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Jonathan Wieland",
                "Hyunsung Cho",
                "Sebastian Hubenschmid",
                "Akihiro Kiuchi",
                "Harald Reiterer",
                "David Lindlbauer"
            ],
            "Name": "Push2AR: Enhancing Mobile List Interactions Using Augmented Reality",
            "Bibtex": "@INPROCEEDINGS{10765366,\n  author={Wieland, Jonathan and Cho, Hyunsung and Hubenschmid, Sebastian and Kiuchi, Akihiro and Reiterer, Harald and Lindlbauer, David},\n  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \n  title={Push2AR: Enhancing Mobile List Interactions Using Augmented Reality}, \n  year={2024},\n  volume={},\n  number={},\n  pages={671-680},\n  keywords={Headphones;Visualization;Navigation;Social networking (online);Prototypes;User interfaces;User experience;Synchronization;Smart phones;Augmented reality;Augmented reality;mobile devices;cross-device interaction},\n  doi={10.1109/ISMAR62088.2024.00082}\n}",
            "DOI": "10.1109/ISMAR62088.2024.00082",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Asymmetric"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Other / None",
                "Study"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Shuqi Liao",
                "Sparsh Chaudhri",
                "Maanas K. Karwa",
                "Voicu Popescu"
            ],
            "Name": "SeamlessVR: Bridging the Immersive to Non-Immersive Visualization Divide",
            "Bibtex": "@ARTICLE{10918852,\n  author={Liao, Shuqi and Chaudhri, Sparsh and Karwa, Maanas K. and Popescu, Voicu},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={SeamlessVR: Bridging the Immersive to Non-Immersive Visualization Divide}, \n  year={2025},\n  volume={31},\n  number={5},\n  pages={2806-2816},\n  keywords={Visualization;Headphones;Three-dimensional displays;Switches;Data visualization;Virtual environments;Target tracking;Usability;User experience;Training;Transitional interface;cross-reality;VR headset removal},\n  doi={10.1109/TVCG.2025.3549564}\n}",
            "DOI": "10.1109/TVCG.2025.3549564",
            "MR Devices": [
                "AR VST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Asymmetric",
                "Migratory Interface"
            ],
            "Temporal": [
                "Exclusive"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "SciVis"
            ],
            "Terminology": [
                "Term: Transitional"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Rasmus Lunding",
                "Sebastian Hubenschmid",
                "Tiare Feuchtner",
                "Kaj Gr\u00f8nb\u00e6k"
            ],
            "Name": "ARTHUR: authoring human\u2013robot collaboration processes with augmented reality using hybrid user interfaces",
            "Bibtex": "@article{Lunding2025,\n  title = {ARTHUR: authoring human\u2013robot collaboration processes with augmented reality using hybrid user interfaces},\n  volume = {29},\n  ISSN = {1434-9957},\n  url = {http://dx.doi.org/10.1007/s10055-025-01149-6},\n  DOI = {10.1007/s10055-025-01149-6},\n  number = {2},\n  journal = {Virtual Reality},\n  publisher = {Springer Science and Business Media LLC},\n  author = {Lunding,  Rasmus and Hubenschmid,  Sebastian and Feuchtner,  Tiare and Gr\u00f8nb\u00e6k,  Kaj},\n  year = {2025},\n  month = apr \n}",
            "DOI": "10.1007/s10055-025-01149-6",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop",
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None",
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Hybrid UI"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Muhammad Raza",
                "Vachiraporn Ketsoi",
                "Joseph Malloch",
                "Saman Bashbaghi",
                "Hakimeh Purmehdi",
                "Derek Reilly"
            ],
            "Name": "PerspectAR: Addressing Perspective Distortion on Very Large Displays with Adaptive Augmented Reality Overlays",
            "Bibtex": "@inproceedings{10.1145/3706598.3713389,\n   author = {Raza, Muhammad and Ketsoi, Vachiraporn and Malloch, Joseph and Bashbaghi, Saman and Purmehdi, Hakimeh and Reilly, Derek},\n   title = {PerspectAR: Addressing Perspective Distortion on Very Large Displays with Adaptive Augmented Reality Overlays},\n   year = {2025},\n   isbn = {9798400713941},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3706598.3713389},\n   doi = {10.1145/3706598.3713389},\n   abstract = {We present PerspectAR, a novel system addressing perspective distortion on displays caused by large size and wide viewing angles. PerspectAR has three components: a virtual AR screen that curves dynamically according to a user\u2019s position relative to the display, a sliding transparent window giving unobstructed access to the physical display in front of the user, and gaze indicators to assist collaborators when they are looking at different renderings. In a within-subjects study in a semi-controlled public environment with 12 pairs, we compared physical display-only and PerspectAR configurations for data analysis tasks. Participants reported less physical workload with PerspectAR and spent more time near the physical display without compromising task performance. Feedback indicates that PerspectAR addressed perspective distortion and provided a contextual view that was useful as a memory aid. Due to the virtual screen curvature, PerspectAR was seen as less effective for tasks involving distance estimates between objects.},\n   booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},\n   articleno = {34},\n   numpages = {21},\n   keywords = {large display, augmented reality, perspective compensation, perspective distortion, co-located collaboration},\n   location = {},\n   series = {CHI '25}\n}",
            "DOI": "10.1145/3706598.3713389",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Large Display"
            ],
            "Configuration": [
                "Augmented Display"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Multi-user - Shared Component"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Sebastian Hubenschmid",
                "Johannes Zagermann",
                "Robin Erb",
                "Tiare Feuchtner",
                "Jens Grubert",
                "Markus Tatzgern",
                "Dieter Schmalstieg",
                "Harald Reiterer"
            ],
            "Name": "SpatialMouse: A Hybrid Pointing Device for Seamless Interaction Across 2D and 3D Spaces",
            "Bibtex": "@inproceedings{10.1145/3756884.3766047,\n   author = {Hubenschmid, Sebastian and Zagermann, Johannes and Erb, Robin and Feuchtner, Tiare and Grubert, Jens and Tatzgern, Markus and Schmalstieg, Dieter and Reiterer, Harald},\n   title = {SpatialMouse: A Hybrid Pointing Device for Seamless Interaction Across 2D and 3D Spaces},\n   year = {2025},\n   isbn = {9798400721182},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3756884.3766047},\n   doi = {10.1145/3756884.3766047},\n   abstract = {We introduce the SpatialMouse, a hybrid pointing device that combines the capabilities of a desktop mouse with the spatial input of a virtual reality (VR) controller, enabling seamless transitions between 2D and 3D interaction spaces in immersive mixed reality environments. Holistic usage scenarios in mixed reality involve tasks suited alternately to 2D or 3D information spaces. Yet, existing input devices excel in either 2D or 3D, but not both, making it necessary to switch between multiple input devices (e.g., mouse and VR controller). Our SpatialMouse addresses this issue, offering the affordances of a desktop mouse for indirect 2D pointing and the spatial capabilities of VR controllers with six degrees of freedom. In a user study with 12&nbsp;participants, our prototype significantly reduced perceived task load and improved user experience compared to switching between separate devices. We extract design recommendations to further support such hybrid input approaches.},\n   booktitle = {Proceedings of the 2025 31st ACM Symposium on Virtual Reality Software and Technology},\n   articleno = {1},\n   numpages = {13},\n   keywords = {mixed reality, cross reality, hybrid interaction, transitional interfaces, input device},\n   location = {},\n   series = {VRST '25}\n}",
            "DOI": "10.1145/3756884.3766047",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Migratory Interface"
            ],
            "Temporal": [
                "Parallel",
                "Exclusive"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional",
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "3D Object Manipulation"
            ],
            "Terminology": [
                "Term: Hybrid <other>"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Demonstration"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2016",
            "Authors": [
                "J\u00e9r\u00e9my Lacoche",
                "Thierry Duval",
                "Bruno Arnaldi",
                "Eric Maisel",
                "J\u00e9r\u00f4me Royan"
            ],
            "Name": "D3part: A new model for redistribution and plasticity of 3d user interfaces",
            "Bibtex": "@INPROCEEDINGS{7460026,\n  author={Lacoche, J\u00e9r\u00e9my and Duval, Thierry and Arnaldi, Bruno and Maisel, Eric and Royan, J\u00e9r\u00f4me},\n  booktitle={2016 IEEE Symposium on 3D User Interfaces (3DUI)}, \n  title={D3PART: A new model for redistribution and plasticity of 3D user interfaces}, \n  year={2016},\n  volume={},\n  number={},\n  pages={23-26},\n  keywords={Three-dimensional displays;Virtual environments;Adaptation models;User interfaces;Context;Solid modeling;Runtime;Plasticity;Redistribution;3D User Interfaces},\n  doi={10.1109/3DUI.2016.7460026}\n}",
            "DOI": "10.1109/3DUI.2016.7460026",
            "MR Devices": [
                "CAVE"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Asymmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Development/Authoring",
                "3D Design/Sketching"
            ],
            "Terminology": [
                "Other Terms"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Jiawei Li",
                "Linjie Qiu",
                "Zhiqing Wu",
                "Qiongyan Chen",
                "Ziyan Wang",
                "Mingming Fan"
            ],
            "Name": "ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance",
            "Bibtex": "@inproceedings{Li_2025, series={MM \u201925},\n   title={ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance},\n   url={http://dx.doi.org/10.1145/3746027.3755578},\n   DOI={10.1145/3746027.3755578},\n   booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},\n   publisher={ACM},\n   author={Li, Jiawei and Qiu, Linjie and Wu, Zhiqing and Chen, Qiongyan and Wang, Ziyan and Fan, Mingming},\n   year={2025},\n   month=oct, pages={7016\u20137025},\n   collection={MM \u201925} \n}",
            "DOI": "10.48550/arXiv.2508.01282",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Symmetric",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Xiaofu Jin",
                "Wai Tong",
                "Xiaoying Wei",
                "Xian Wang",
                "Emily Kuang",
                "Xiaoyu Mo",
                "Huamin Qu",
                "Mingming Fan"
            ],
            "Name": "Exploring the opportunity of augmented reality (AR) in supporting older adults to explore and learn smartphone applications",
            "Bibtex": "@inproceedings{10.1145/3613904.3641901,\n   author = {Jin, Xiaofu and Tong, Wai and Wei, Xiaoying and Wang, Xian and Kuang, Emily and Mo, Xiaoyu and Qu, Huamin and Fan, Mingming},\n   title = {Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults to Explore and Learn Smartphone Applications},\n   year = {2024},\n   isbn = {9798400703300},\n   publisher = {Association for Computing Machinery},\n   address = {New York, NY, USA},\n   url = {https://doi.org/10.1145/3613904.3641901},\n   doi = {10.1145/3613904.3641901},\n   abstract = {The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications. While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults\u2019 smartphone app exploration remains insufficiently explored. Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools. Our research highlights AR\u2019s effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process. We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration. Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios.},\n   booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\n   articleno = {21},\n   numpages = {18},\n   keywords = {augmented reality, independent learning, older adults, smartphone exploration},\n   location = {Honolulu, HI, USA},\n   series = {CHI '24}\n}",
            "DOI": "10.1145/3613904.3641901",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Symmetric",
                "VESAD",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "Other / None"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Mat\u011bj Lang",
                "Clemens Strobel",
                "Felix Weckesser",
                "Danielle Langlois",
                "Enkelejda Kasneci",
                "Barbora Kozl\u00edkov\u00e1",
                "Michael Krone"
            ],
            "Name": "A multimodal smartwatch-based interaction concept for immersive environments",
            "Bibtex": "@article{LANG202385,\n   title = {A multimodal smartwatch-based interaction concept for immersive environments},\n   journal = {Computers & Graphics},\n   volume = {117},\n   pages = {85-95},\n   year = {2023},\n   issn = {0097-8493},\n   doi = {https://doi.org/10.1016/j.cag.2023.10.010},\n   url = {https://www.sciencedirect.com/science/article/pii/S0097849323002479},\n   author = {Mat\u011bj Lang and Clemens Strobel and Felix Weckesser and Danielle Langlois and Enkelejda Kasneci and Barbora Kozl\u00edkov\u00e1 and Michael Krone},\n   keywords = {Smartwatch, AR user interaction, Gesture interface, Input device, User experience, Cross-device interaction},\n   abstract = {Augmented and Virtual Reality (AR/VR) environments require user interaction concepts beyond the traditional mouse-and-keyboard setup for seated desktop computer usage. Although advanced input modalities such as hand or gaze tracking have been developed, they have yet to be widely adopted in available hardware. Modern smartwatches have been shown to provide a powerful and intuitive means of input, thereby overcoming the limitation of the current AR/VR headsets. They typically offer a set of interesting input modalities, such as a touchscreen, rotary buttons, and an Inertial Measurement Unit (IMU), which can be used for mid-air gesture recognition. Compared to other input devices, they have the benefit that they are hands-free as soon as the user stops interacting since they are attached to the wrist. As many concepts have been proposed, comparative evaluations of their effectiveness and user-friendliness are still rare. In this paper, we evaluate the usability of two commonly found approaches for using a smartwatch as an interaction device, specifically in immersive environments provided by AR/VR HMDs: using the physical inputs of the watch (touchscreen, rotary buttons) or mid-air gestures. We conducted a user study with 20 participants, where they tested both of the interaction methods, and we compared them in their usability and performance. Based on a prototypical AR application, we evaluated the performance and user experience of these two smartwatch-based interaction concepts. We have found that the input using a touchscreen and buttons was generally favored by the participants and led to shorter task completion times.}\n}",
            "DOI": "10.1016/j.cag.2023.10.010",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartwatch"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Medical",
                "3D Object Manipulation",
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Tim Menzner",
                "Travis Gesslein",
                "Alexander Otte",
                "Jens Grubert"
            ],
            "Name": "Above surface interaction for multiscale navigation in mobile virtual reality",
            "Bibtex": "@INPROCEEDINGS{9089611,\n  author={Menzner, Tim and Gesslein, Travis and Otte, Alexander and Grubert, Jens},\n  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \n  title={Above Surface Interaction for Multiscale Navigation in Mobile Virtual Reality}, \n  year={2020},\n  volume={},\n  number={},\n  pages={372-381},\n  keywords={Navigation;Two dimensional displays;Three-dimensional displays;Sensors;Aerospace electronics;Virtual reality;User interfaces;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design;evaluation methods},\n  doi={10.1109/VR46266.2020.00057}\n}",
            "DOI": "10.1109/VR46266.2020.00057",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "Other / None",
                "Study"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2025",
            "Authors": [
                "Fengyuan Zhu",
                "Xun Qian",
                "Daniel Kalmar",
                "Mahdi Tayarani",
                "Eric J. Gonzalez",
                "Mar Gonzalez-Franco"
            ],
            "Name": "Beyond the Phone: Exploring Phone-XR Integration through Multi-View Transitions for Real-World Applications",
            "Bibtex": "@INPROCEEDINGS{10937396,\n  author={Zhu, Fengyuan and Qian, Xun and Kalmar, Daniel and Tayarani, Mahdi and Gonzalez, Eric J. and Gonzalez-Franco, Mar and Kim, David and Du, Ruofei},\n  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, \n  title={Beyond the Phone: Exploring Phone-XR Integration through Multi-View Transitions for Real-World Applications}, \n  year={2025},\n  volume={},\n  number={},\n  pages={770-780},\n  keywords={Technological innovation;Solid modeling;Three-dimensional displays;Runtime;User centered design;Prototypes;User interfaces;Real-time systems;Smart phones;Systematic literature review;Cross-Device Interaction;Phone-XR Intergration},\n  doi={10.1109/VR59515.2025.00099}\n}",
            "DOI": "10.1109/VR59515.2025.00099",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control",
                "Asymmetric"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None",
                "3D Object Manipulation",
                "Productivity",
                "Entertainment"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Other Terms"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [
                "Empirical (-)"
            ],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Verena Biener",
                "Daniel Schneider",
                "Travis Gesslein",
                "Alexander Otte",
                "Bastian Kuth",
                "Per Ola Kristensson"
            ],
            "Name": "Breaking the screen: Interaction across touchscreen boundaries in virtual reality for mobile knowledge workers",
            "Bibtex": "@ARTICLE{9212653,\n  author={Biener, Verena and Schneider, Daniel and Gesslein, Travis and Otte, Alexander and Kuth, Bastian and Kristensson, Per Ola and Ofek, Eyal and Pahud, Michel and Grubert, Jens},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers}, \n  year={2020},\n  volume={26},\n  number={12},\n  pages={3490-3502},\n  keywords={Three-dimensional displays;Two dimensional displays;Virtual reality;Microsoft Windows;User interfaces;Touch sensitive screens;Sensors;virtual reality;knowledge work;mobile office;window management;eye tracking;multimodal interaction},\n  doi={10.1109/TVCG.2020.3023567}\n}",
            "DOI": "10.1109/TVCG.2020.3023567",
            "MR Devices": [
                "VR HWD"
            ],
            "2D Devices": [
                "Tablet"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Other / None",
                "SciVis",
                "Study",
                "Productivity",
                "DataVis/Data Analysis",
                "Medical",
                "Development/Authoring"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Empirical",
                "Artifact"
            ],
            "Secondary Contribution": [
                "Theory (-)"
            ],
            "Evaluation": [
                "Usage",
                "Demonstration"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2023",
            "Authors": [
                "Georgios Papadoulis",
                "Christos Sintoris",
                "Christos Fidas",
                "Nikolaos Avouris"
            ],
            "Name": "Extending User Interaction with Mixed Reality Through a Smartphone-Based Controller",
            "Bibtex": "@InProceedings{10.1007/978-3-031-42280-5_27,\n  author=\"Papadoulis, Georgios\n  and Sintoris, Christos\n  and Fidas, Christos\n  and Avouris, Nikolaos\",\n  editor=\"Abdelnour Nocera, Jos{\\'e}\n  and Krist{\\'i}n L{\\'a}rusd{\\'o}ttir, Marta\n  and Petrie, Helen\n  and Piccinno, Antonio\n  and Winckler, Marco\",\n  title=\"Extending User Interaction with\u00a0Mixed Reality Through a\u00a0Smartphone-Based Controller\",\n  booktitle=\"Human-Computer Interaction -- INTERACT 2023\",\n  year=\"2023\",\n  publisher=\"Springer Nature Switzerland\",\n  address=\"Cham\",\n  pages=\"426--435\",\n  abstract=\"A major concern in mixed-reality (MR) environments is to support intuitive and precise user interaction. Various modalities have been proposed and used, including gesture, gaze, voice, hand-recognition or even special devices, i.e. external controllers. However, these modalities may often feel unfamiliar and physically demanding to the end-user, leading to difficulties and fatigue. One possible solution worth investigating further is to use an everyday object, like a smartphone, as an external device for interacting with MR. In this paper, we present the design of a framework for developing an external smartphone controller to extend user input in MR applications, which we further utilize to implement a new interaction modality, a tap on the phone. We also report on findings of a user study (n=24) in which we examine performance and user experience of the suggested input modality through a comparative user evaluation task. The findings suggest that incorporating a smartphone as an external controller shows potential for enhancing user interaction in MR tasks requiring high precision, as well as pinpointing the value of providing alternative means of user input in MR applications depending on a given task and personalization aspects of an end-user.\",\n  isbn=\"978-3-031-42280-5\"\n}",
            "DOI": "10.1007/978-3-031-42280-5_27",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (MR-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Study"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "Informative"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Xin Zeng",
                "Xiaoyu Wang",
                "Tengxiang Zhang",
                "Yukang Yan",
                "Yiqiang Chen"
            ],
            "Name": "InputJump: Augmented reality-facilitated cross-device input fusion based on spatial and semantic information",
            "Bibtex": "@article{ZENG2024502,\n  title = {InputJump: Augmented reality-facilitated cross-device input fusion based on spatial and semantic information},\n  journal = {Virtual Reality & Intelligent Hardware},\n  volume = {6},\n  number = {6},\n  pages = {502-526},\n  year = {2024},\n  issn = {2096-5796},\n  doi = {https://doi.org/10.1016/j.vrih.2024.10.001},\n  url = {https://www.sciencedirect.com/science/article/pii/S2096579624000639},\n  author = {Xin Zeng and Xiaoyu Wang and Tengxiang Zhang and Yukang Yan and Yiqiang Chen},\n  keywords = {Mixed reality, Spatial, Semantic, Large language models},\n  abstract = {The proliferation of computing devices requires seamless cross-device interactions. Augmented reality (AR) headsets can facilitate interactions with existing computers owing to their user-centered views and natural inputs. In this study, we propose InputJump, a user-centered cross-device input fusion method that maps multi-modal cross-device inputs to interactive elements on graphical interfaces. The input jump calculates the spatial coordinates of the input target positions and the interactive elements within the coordinate system of the AR headset. It also extracts semantic descriptions of inputs and elements using large language models (LLMs). Two types of information from different inputs (e.g., gaze, gesture, mouse, and keyboard) were fused to map onto an interactive element. The proposed method is explained in detail and implemented on both an AR headset and a desktop PC. We then conducted a user study and extensive simulations to validate our proposed method. The results showed that InputJump can accurately associate a fused input with the target interactive element, enabling a more natural and flexible interaction experience.}\n}",
            "DOI": "10.1016/j.vrih.2024.10.001",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Remote Control"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Flexible"
            ],
            "Interaction Dynamics": [
                "Bidirectional"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "Yes"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Hyunsung Cho",
                "Yukang Yan",
                "Kashyap Todi",
                "Mark Parent",
                "Missie Smith",
                "Tanya R. Jonker",
                "Hrvoje Benko",
                "David Lindlbauer"
            ],
            "Name": "Minexr: Mining personalized extended reality interfaces",
            "Bibtex": "@inproceedings{10.1145/3613904.3642394,\n  author = {Cho, Hyunsung and Yan, Yukang and Todi, Kashyap and Parent, Mark and Smith, Missie and Jonker, Tanya R. and Benko, Hrvoje and Lindlbauer, David},\n  title = {MineXR: Mining Personalized Extended Reality Interfaces},\n  year = {2024},\n  isbn = {9798400703300},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3613904.3642394},\n  doi = {10.1145/3613904.3642394},\n  abstract = {Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce &nbsp;MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data. &nbsp;MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using &nbsp;MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces.},\n  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\n  articleno = {609},\n  numpages = {17},\n  keywords = {Datasets, Extended Reality, Personalized UI},\n  location = {Honolulu, HI, USA},\n  series = {CHI '24}\n}",
            "DOI": "10.1145/3613904.3642394",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Logical Distribution"
            ],
            "Temporal": [
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Social"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "Development/Authoring"
            ],
            "Terminology": [
                "Term: Cross-Device",
                "Undefined"
            ],
            "Main Contribution": [
                "Dataset"
            ],
            "Secondary Contribution": [
                "Artifact (-)"
            ],
            "Evaluation": [
                "No Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2021",
            "Authors": [
                "Arda Ege Unlu",
                "Robert Xiao"
            ],
            "Name": "PAIR: Phone as an augmented immersive reality controller",
            "Bibtex": "@inproceedings{10.1145/3489849.3489878,\n  author = {Unlu, Arda Ege and Xiao, Robert},\n  title = {PAIR: Phone as an Augmented Immersive Reality Controller},\n  year = {2021},\n  isbn = {9781450390927},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3489849.3489878},\n  doi = {10.1145/3489849.3489878},\n  abstract = {Immersive head-mounted augmented reality allows users to overlay 3D digital content on a user\u2019s view of the world. Current-generation devices primarily support interaction modalities such as gesture, gaze and voice, which are readily available to most users yet lack precision and tactility, rendering them fatiguing for extended interactions. We propose using smartphones, which are also readily available, as companion devices complementing existing AR interaction modalities. We leverage user familiarity with smartphone interactions, coupled with their support for precise, tactile touch input, to unlock a broad range of interaction techniques and applications - for instance, turning the phone into an interior design palette, touch-enabled catapult or AR-rendered sword. We describe a prototype implementation of our interaction techniques using an off-the-shelf AR headset and smartphone, demonstrate applications, and report on the results of a positional accuracy study.},\n  booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},\n  articleno = {27},\n  numpages = {6},\n  keywords = {Touch, Smartphone, Input Techniques, Augmented Reality},\n  location = {Osaka, Japan},\n  series = {VRST '21}\n}",
            "DOI": "10.1145/3489849.3489878",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control",
                "Logical Distribution"
            ],
            "Temporal": [
                "Parallel",
                "Serial"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Dynamic"
            ],
            "Use Case": [
                "3D Object Manipulation",
                "Entertainment",
                "Development/Authoring"
            ],
            "Terminology": [
                "Undefined"
            ],
            "Main Contribution": [
                "Artifact"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Technical Evaluation"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2020",
            "Authors": [
                "Xiyao Wang",
                "Lonni Besan\u00e7on",
                "David Rousseau",
                "Mickael Sereno",
                "Mehdi Ammi",
                "Tobias Isenberg"
            ],
            "Name": "Towards an understanding of augmented reality extensions for existing 3D data analysis tools",
            "Bibtex": "@inproceedings{10.1145/3313831.3376657,\n  author = {Wang, Xiyao and Besan\\c{c}on, Lonni and Rousseau, David and Sereno, Mickael and Ammi, Mehdi and Isenberg, Tobias},\n  title = {Towards an Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools},\n  year = {2020},\n  isbn = {9781450367080},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3313831.3376657},\n  doi = {10.1145/3313831.3376657},\n  abstract = {We present an observational study with domain experts to understand how augmented reality (AR) extensions to traditional PC-based data analysis tools can help particle physicists to explore and understand 3D data. Our goal is to allow researchers to integrate stereoscopic AR-based visual representations and interaction techniques into their tools, and thus ultimately to increase the adoption of modern immersive analytics techniques in existing data analysis workflows. We use Microsoft's HoloLens as a lightweight and easily maintainable AR headset and replicate existing visualization and interaction capabilities on both the PC and the AR view. We treat the AR headset as a second yet stereoscopic screen, allowing researchers to study their data in a connected multi-view manner. Our results indicate that our collaborating physicists appreciate a hybrid data exploration setup with an interactive AR extension to improve their understanding of particle collision events.},\n  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\n  pages = {1\u201313},\n  numpages = {13},\n  keywords = {3D visualization, hybrid visualization system, immersive analytics, user interface},\n  location = {Honolulu, HI, USA},\n  series = {CHI '20}\n}",
            "DOI": "10.1145/3313831.3376657",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Desktop"
            ],
            "Configuration": [
                "Dynamic Lens"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Personal"
            ],
            "Device Dependency": [
                "Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Component-coupled"
            ],
            "Use Case": [
                "DataVis/Data Analysis",
                "SciVis"
            ],
            "Terminology": [
                "Term: Hybrid <other>",
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        },
        {
            "Year": "2024",
            "Authors": [
                "Xiaotian Zhang",
                "Weiping He",
                "Mark Billinghurst",
                "Daisong Liu",
                "Lingxiao Yang",
                "Shuo Feng",
                "Yizhe Liu"
            ],
            "Name": "Usability of cross-device interaction interfaces for augmented reality in physical tasks",
            "Bibtex": "@article{Zhang02052024,\n  author = {Xiaotian Zhang and Weiping He and Mark Billinghurst and Daisong Liu and Lingxiao Yang and Shuo Feng and Yizhe Liu},\n  title = {Usability of Cross-Device Interaction Interfaces for Augmented Reality in Physical Tasks},\n  journal = {International Journal of Human\u2013Computer Interaction},\n  volume = {40},\n  number = {9},\n  pages = {2361--2379},\n  year = {2024},\n  publisher = {Taylor \\& Francis},\n  doi = {10.1080/10447318.2022.2160537}\n}",
            "DOI": "10.1080/10447318.2022.2160537",
            "MR Devices": [
                "AR OST HWD"
            ],
            "2D Devices": [
                "Smartwatch",
                "Smartphone"
            ],
            "Configuration": [
                "Remote Control"
            ],
            "Temporal": [
                "Parallel"
            ],
            "Relationship": [
                "Single User"
            ],
            "Range": [
                "Near",
                "Personal"
            ],
            "Device Dependency": [
                "Semi-Fixed"
            ],
            "Interaction Dynamics": [
                "Unidirectional (2D-centric)"
            ],
            "Space": [
                "Co-Located"
            ],
            "Anchoring": [
                "Free"
            ],
            "Use Case": [
                "Productivity",
                "Study"
            ],
            "Terminology": [
                "Term: Cross-Device"
            ],
            "Main Contribution": [
                "Empirical"
            ],
            "Secondary Contribution": [],
            "Evaluation": [
                "Informative",
                "Usage"
            ],
            "Edge Case": [
                "No"
            ]
        }
    ]
}